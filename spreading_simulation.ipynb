{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn import svm\n",
    "from pandas import *\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "def load_pickle_file(pickled_file):\n",
    "    print(f'Loading data file from {pickled_file}')\n",
    "    infile = open(pickled_file,'rb')\n",
    "    unpickled_file = pickle.load(infile)\n",
    "    print(f'Loaded {len(unpickled_file)} entries')\n",
    "    infile.close()\n",
    "    return unpickled_file\n",
    "          \n",
    "    \n",
    "def save_pickle_file(path, data):\n",
    "    print('Dumping data to path {}'.format(path))\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print('Finished dumping data to path {}'.format(path))\n",
    "\n",
    "\n",
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "\n",
    "def safe_division(x, y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path = \"F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\\"\n",
    "# event = 'givenchy'\n",
    "# interval = 30\n",
    "# current_time = 360\n",
    "# start_hour = 6\n",
    "\n",
    "    \n",
    "# users = load_pickle_file(path+\"users.dat\")\n",
    "# users.reset_index(drop =True , inplace =True)\n",
    "# # print(users)\n",
    "# print(users.columns)\n",
    "# print(users['source_candidates'])\n",
    "\n",
    "\n",
    "# print(unique_users)\n",
    "def id_exists(unique_users, uid):\n",
    "    if uid in unique_users:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_id_list(unique_users, value):\n",
    "        return list(filter(lambda uid: id_exists(unique_users,uid), value))\n",
    "    \n",
    "\n",
    "def network_simulation_init(users, simulation_start_min):\n",
    "    current_time = simulation_start_min\n",
    "    network_simulation = pd.DataFrame(columns= ['id','time_lapsed','favourites_count','followers_count','friends_count',\n",
    "                                'listed_count','statuses_count','source_candidates','source_index','seed_index','generation',\n",
    "                                 'time_since_seed','user_created_days','normalized_statuses_count','normalized_followers_count',\n",
    "                                 'normalized_favourites_count','normalized_listed_count','normalized_friends_count'])\n",
    "\n",
    "    network_simulation['id']=users['id']\n",
    "    network_simulation['favourites_count']=users['favourites_count']\n",
    "    network_simulation['followers_count']=users['followers_count']\n",
    "    network_simulation['friends_count']=users['friends_count']\n",
    "    network_simulation['listed_count']=users['listed_count']\n",
    "    network_simulation['statuses_count']=users['statuses_count']\n",
    "    network_simulation['user_created_days']=users['user_created_days']\n",
    "    network_simulation['normalized_statuses_count']=users['normalized_statuses_count']\n",
    "    network_simulation['normalized_followers_count']=users['normalized_followers_count']\n",
    "    network_simulation['normalized_favourites_count']=users['normalized_favourites_count']\n",
    "    network_simulation['normalized_listed_count']=users['normalized_listed_count']\n",
    "    network_simulation['normalized_friends_count']=users['normalized_friends_count']\n",
    "#     network_simulation['followers_list'] =users['followers_json'] \n",
    "#     network_simulation['friends_list'] =users['friends_json']\n",
    "    network_simulation['time_lapsed'] = users['time_lapsed'].apply(lambda x: x if x <= current_time else None)\n",
    "    network_simulation['source_index'] = users.apply(lambda x: x['source_index'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['seed_index'] = users.apply(lambda x: x['seed_index'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['generation'] = users.apply(lambda x: x['generation'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['time_since_seed'] = users.apply(lambda x: x['time_since_seed'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "\n",
    "# print(\"network_simulation\")\n",
    "# print(network_simulation)\n",
    "    return network_simulation\n",
    "\n",
    "\n",
    "def network_simulation_merge_friends_list(network_simulation, path_of_friends_list):\n",
    "\n",
    "    friends = load_pickle_file(path_of_friends_list)\n",
    "    friends_list = pd.DataFrame(columns=['id'], data=friends.keys())\n",
    "    friends_list[\"friends_list\"] = ''\n",
    "    for i in range (0, len(friends)):\n",
    "        friends_list[\"friends_list\"][i]= friends.get(friends_list[\"id\"][i])\n",
    "    friends_list.reset_index(drop =True , inplace =True)\n",
    "    return pd.merge(network_simulation,friends_list, on='id', how='left')\n",
    "    \n",
    "\n",
    "def network_simulation_merge_followers_list(network_simulation, path_of_followers_list):\n",
    "    followers = load_pickle_file(path_of_followers_list)\n",
    "    followers_list = pd.DataFrame(columns=['id'], data=followers.keys())\n",
    "    followers_list[\"followers_list\"] = ''\n",
    "    for i in range (0, len(followers)):\n",
    "        followers_list[\"followers_list\"][i]= followers.get(followers_list[\"id\"][i])\n",
    "    # print(friends_list.get(942362499923566592 ))\n",
    "    followers_list.reset_index(drop =True , inplace =True)\n",
    "    # print(followers_list)\n",
    "\n",
    "\n",
    "    return pd.merge(network_simulation,followers_list,on='id',how='left')\n",
    "    \n",
    "#     print(network_simulation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# construct and extract features dataset based on the information in users and  network_simulation data.  \n",
    "# current_min is number of minutes since the first seed, and all imformation before this time point is known.\n",
    "# This function will only generate features for users between start_index and end_index. If current_features is not None, \n",
    "# the function will update the features for users between start_index and end_index.\n",
    "def construct_features(users, network_simulation, start_index, end_index, current_min, current_features=None):\n",
    "    \n",
    "    current_time = current_min   \n",
    "    features = current_features\n",
    "    in_degree = list(users.friends_count)\n",
    "    out_degree = list(users.followers_count)\n",
    "    degree = (users.friends_count+users.followers_count).tolist()\n",
    "    if current_features is None:\n",
    "        features = {\n",
    "            #Columns which are added for simulation, but they are not used as features for model prediction\n",
    "            'user_id':[],\n",
    "            'infected_status':[],\n",
    "            'infection_time':[],\n",
    "            'followers_list':[],\n",
    "\n",
    "            #Columns used as features for model prediction   33\n",
    "            'UsM_deltaDays': [],\n",
    "            'UsM_statusesCount': [],\n",
    "            'UsM_followersCount': [],\n",
    "            'UsM_favouritesCount': [],\n",
    "            'UsM_friendsCount': [],\n",
    "            'UsM_listedCount': [],\n",
    "            'UsM_normalizedUserStatusesCount': [],\n",
    "            'UsM_normalizedUserFollowersCount': [],\n",
    "            'UsM_normalizedUserFavouritesCount': [],\n",
    "            'UsM_normalizedUserListedCount': [],\n",
    "            'UsM_normalizedUserFriendsCount': [],          \n",
    "            'UsM_deltaDays0': [],\n",
    "            'UsM_statusesCount0': [],\n",
    "            'UsM_followersCount0': [],\n",
    "            'UsM_favouritesCount0': [],\n",
    "            'UsM_friendsCount0': [],\n",
    "            'UsM_listedCount0': [],\n",
    "            'UsM_normalizedUserStatusesCount0': [],\n",
    "            'UsM_normalizedUserFollowersCount0': [],\n",
    "            'UsM_normalizedUserFavouritesCount0': [],\n",
    "            'UsM_normalizedUserListedCount0': [],\n",
    "            'UsM_normalizedUserFriendsCount0': [],\n",
    "            'UsM_deltaDays-1': [],\n",
    "            'UsM_statusesCount-1': [],\n",
    "            'UsM_followersCount-1': [],\n",
    "            'UsM_favouritesCount-1': [],\n",
    "            'UsM_friendsCount-1': [],\n",
    "            'UsM_listedCount-1': [],\n",
    "            'UsM_normalizedUserStatusesCount-1': [],\n",
    "            'UsM_normalizedUserFollowersCount-1': [],\n",
    "            'UsM_normalizedUserFavouritesCount-1': [],\n",
    "            'UsM_normalizedUserListedCount-1': [],\n",
    "            'UsM_normalizedUserFriendsCount-1': [],\n",
    "            # TwM: Tweet metadata   5\n",
    "            'TwM_t0': [],\n",
    "            'TwM_tSeed0': [],\n",
    "            'TwM_t-1': [],\n",
    "            'TwM_tSeed-1': [],\n",
    "            'TwM_tCurrent': [],\n",
    "            # Nw: Network0    15\n",
    "            'Nw_degree': [],\n",
    "            'Nw_inDegree': [],\n",
    "            'Nw_outDegree': [],\n",
    "            'Nw_degree0': [],\n",
    "            'Nw_inDegree0': [],\n",
    "            'Nw_outDegree0': [],\n",
    "            'Nw_degree-1': [],\n",
    "            'Nw_inDegree-1': [],\n",
    "            'Nw_outDegree-1': [],\n",
    "            'Nw_degreeSeed0': [],\n",
    "            'Nw_inDegreeSeed0': [],\n",
    "            'Nw_outDegreeSeed0': [],\n",
    "            'Nw_degreeSeed-1': [],\n",
    "            'Nw_inDegreeSeed-1': [],\n",
    "            'Nw_outDegreeSeed-1': [],\n",
    "            # SNw: Spreading Network   20\n",
    "            'SNw_nFriendsInfected': [],\n",
    "            'SNw_friendsInfectedRatio': [],\n",
    "            'SNw_generation0': [],\n",
    "            'SNw_generation-1': [],\n",
    "            'SNw_timeSinceSeed0': [],\n",
    "            'SNw_timeSinceSeed-1': [],\n",
    "            'SNw_totalNodesInfected': [],\n",
    "            'SNw_nodeInfectedCentrality': [],\n",
    "            'SNw_totalInDegree': [],\n",
    "            'SNw_totalOutDegree': [],\n",
    "            'SNw_inDegreeCentrality': [],\n",
    "            'SNw_inDegreeCentrality0': [],\n",
    "            'SNw_inDegreeCentrality-1': [],\n",
    "            'SNw_outDegreeCentrality': [],\n",
    "            'SNw_outDegreeCentrality0': [],\n",
    "            'SNw_outDegreeCentrality-1': [],\n",
    "            'SNw_inDegreeCentralitySeed0':[],\n",
    "            'SNw_outDegreeCentralitySeed0':[],\n",
    "            'SNw_inDegreeCentralitySeed-1':[],\n",
    "            'SNw_outDegreeCentralitySeed-1':[],\n",
    "            # Stat: Statistical  15\n",
    "            'Stat_average_kOut': [],\n",
    "            'Stat_average_t': [],\n",
    "            'Stat_average_deltaDays': [],\n",
    "            'Stat_average_statusesCount': [],\n",
    "            'Stat_average_followersCount': [],\n",
    "            'Stat_average_favouritesCount': [],\n",
    "            'Stat_average_friendsCount': [],\n",
    "            'Stat_average_listedCount': [],\n",
    "            'Stat_average_normalizedUserStatusesCount': [],\n",
    "            'Stat_average_normalizedUserFollowersCount': [],\n",
    "            'Stat_average_normalizedUserFavouritesCount': [],\n",
    "            'Stat_average_normalizedUserListedCount': [],\n",
    "            'Stat_average_normalizedUserFriendsCount': [],                \n",
    "            'Stat_max_kOut': [],\n",
    "            'Stat_min_kOut': []\n",
    "\n",
    "        }\n",
    "\n",
    "        with tqdm(total=len(list(users[start_index: end_index].iterrows()))) as pbar: \n",
    "            for index, user_row in users[start_index: end_index].iterrows():\n",
    "                source_candidates = sorted(user_row['source_candidates'])\n",
    "                features['user_id'].append(user_row['id'])\n",
    "                features['infected_status'].append(False)\n",
    "                features['infection_time'].append(None)\n",
    "                #print(f\"user_row['followers_list']:{user_row['followers_list']}\")\n",
    "                features['followers_list'].append(user_row['followers_json'])\n",
    "                #print(\"b\")\n",
    "                features['UsM_deltaDays'].append(user_row['user_created_days'])\n",
    "                features['UsM_statusesCount'].append(user_row['statuses_count'])\n",
    "                features['UsM_followersCount'].append(user_row['followers_count'])\n",
    "                features['UsM_favouritesCount'].append(user_row['favourites_count'])\n",
    "                features['UsM_friendsCount'].append(user_row['friends_count'])\n",
    "                features['UsM_listedCount'].append(user_row['listed_count'])\n",
    "                features['UsM_normalizedUserStatusesCount'].append(user_row['normalized_statuses_count'])\n",
    "                features['UsM_normalizedUserFollowersCount'].append(user_row['normalized_followers_count'])\n",
    "                features['UsM_normalizedUserFavouritesCount'].append(user_row['normalized_favourites_count'])\n",
    "                features['UsM_normalizedUserListedCount'].append(user_row['normalized_listed_count'])\n",
    "                features['UsM_normalizedUserFriendsCount'].append(user_row['normalized_friends_count'])\n",
    "                if user_row['time_lapsed'] <= current_time:\n",
    "\n",
    "                    features['infected_status'][-1] = True\n",
    "                    features['infection_time'][-1] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[index,'time_lapsed'] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[index,'source_index'] = user_row['source_index']\n",
    "                    network_simulation.loc[index,'seed_index'] = user_row['seed_index']\n",
    "                    network_simulation.loc[index, 'generation'] = user_row['generation']\n",
    "                    network_simulation.loc[index, 'time_since_seed'] = user_row['time_since_seed']\n",
    "                if isinstance(source_candidates, list):\n",
    "                    sources = network_simulation.loc[source_candidates]\n",
    "                    sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "    #                 if user_row['time_lapsed'] > current_time:\n",
    "    #                     sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "    #                 else:\n",
    "    #                      sources_dataframe = sources[sources['time_lapsed'] <= user_row['time_lapsed']]\n",
    "                    sources = sources_dataframe.index.tolist()\n",
    "                else:\n",
    "                    sources = []\n",
    "                #sources = [x for x in source_candidates if users.loc[x,'time_lapsed'] <= current_time]\n",
    "                #print(f'sources:{sources}')\n",
    "                if len(sources) > 0:\n",
    "\n",
    "                    # Assign the values here to save computation\n",
    "                    first_source_index = source_candidates[0]\n",
    "                    first_source_row = users.loc[first_source_index]\n",
    "                    first_source_seed_row = users.loc[first_source_row['seed_index']]\n",
    "\n",
    "                    inDegreeList = sources_dataframe.friends_count.tolist()\n",
    "                    outDegreeList = sources_dataframe.followers_count.tolist()\n",
    "                    degreeList = [x + y for x, y in zip(inDegreeList, outDegreeList)]\n",
    "\n",
    "                    s_ind = sources_dataframe.friends_count\n",
    "                    s_outd = sources_dataframe.followers_count\n",
    "                    outDegreeList = s_outd.tolist()\n",
    "                    inDegreeList = s_ind.tolist()\n",
    "                    degreeList = (s_ind + s_outd).tolist()\n",
    "\n",
    "                    #degreeList = list(users.loc[i, 'followers_count'] + users.loc[i, 'friends_count']  for i in sources)\n",
    "                    current_time_series = pd.Series([current_time] * len(sources))\n",
    "                    time_lapsed_series = sources_dataframe.time_lapsed\n",
    "                    timeList = (current_time_series - time_lapsed_series).tolist()\n",
    "\n",
    "\n",
    "                    last_source_index = sources[-1]\n",
    "                    last_source_row = network_simulation.loc[last_source_index]\n",
    "                    last_source_seed_row = network_simulation.loc[last_source_row['seed_index']]\n",
    "\n",
    "                    usr_index = index\n",
    "\n",
    "    #                 if user_row['time_lapsed'] <= current_time:\n",
    "\n",
    "    #                     features['infected_status'][-1] = True\n",
    "    #                     features['infection_time'][-1] = user_row['time_lapsed']\n",
    "\n",
    "    #                     network_simulation.loc[usr_index,'time_lapsed'] = user_row['time_lapsed']\n",
    "\n",
    "    #                     network_simulation.loc[usr_index,'source_index'] = user_row['source_index']\n",
    "    #                     network_simulation.loc[usr_index,'seed_index'] = user_row['seed_index']\n",
    "    #                     network_simulation.loc[usr_index, 'generation'] = user_row['generation']\n",
    "    #                     network_simulation.loc[usr_index, 'time_since_seed'] = user_row['time_since_seed']\n",
    "\n",
    "\n",
    "                    network_simulation.at[usr_index,'source_candidates'] = sources\n",
    "\n",
    "                    # UsM: User metadata\n",
    "\n",
    "                    features['UsM_deltaDays0'].append(first_source_row.user_created_days)\n",
    "                    features['UsM_statusesCount0'].append(first_source_row.statuses_count)\n",
    "                    features['UsM_followersCount0'].append(first_source_row.followers_count)\n",
    "                    features['UsM_favouritesCount0'].append(first_source_row.favourites_count)\n",
    "                    features['UsM_friendsCount0'].append(first_source_row.friends_count)\n",
    "                    features['UsM_listedCount0'].append(first_source_row.listed_count)\n",
    "                    features['UsM_normalizedUserStatusesCount0'].append(first_source_row.normalized_statuses_count)\n",
    "                    features['UsM_normalizedUserFollowersCount0'].append(first_source_row.normalized_followers_count)\n",
    "                    features['UsM_normalizedUserFavouritesCount0'].append(first_source_row.normalized_favourites_count)\n",
    "                    features['UsM_normalizedUserListedCount0'].append(first_source_row.normalized_listed_count)\n",
    "                    features['UsM_normalizedUserFriendsCount0'].append(first_source_row.normalized_friends_count)\n",
    "                    features['UsM_deltaDays-1'].append(last_source_row.user_created_days)\n",
    "                    features['UsM_statusesCount-1'].append(last_source_row.statuses_count)\n",
    "                    features['UsM_followersCount-1'].append(last_source_row.followers_count)\n",
    "                    features['UsM_favouritesCount-1'].append(last_source_row.favourites_count)\n",
    "                    features['UsM_friendsCount-1'].append(last_source_row.friends_count)\n",
    "                    features['UsM_listedCount-1'].append(last_source_row.listed_count)\n",
    "                    features['UsM_normalizedUserStatusesCount-1'].append(last_source_row.normalized_statuses_count)\n",
    "                    features['UsM_normalizedUserFollowersCount-1'].append(last_source_row.normalized_followers_count)\n",
    "                    features['UsM_normalizedUserFavouritesCount-1'].append(last_source_row.normalized_favourites_count)\n",
    "                    features['UsM_normalizedUserListedCount-1'].append(last_source_row.normalized_listed_count)\n",
    "                    features['UsM_normalizedUserFriendsCount-1'].append(last_source_row.normalized_friends_count)\n",
    "                    # TwM: Tweet metadata\n",
    "                    features['TwM_t0'].append(round(timeList[0], 1))\n",
    "                    features['TwM_tSeed0'].append(round(current_time - first_source_seed_row['time_lapsed'], 1))\n",
    "                    features['TwM_t-1'].append(round(timeList[-1], 1))\n",
    "                    features['TwM_tSeed-1'].append(round(current_time - last_source_seed_row['time_lapsed'], 1))\n",
    "                    features['TwM_tCurrent'].append(current_time)\n",
    "                    # Nw: Network\n",
    "                    features['Nw_degree'].append(degree[index])\n",
    "                    features['Nw_inDegree'].append(in_degree[index])\n",
    "                    features['Nw_outDegree'].append(out_degree[index])\n",
    "                    features['Nw_degree0'].append(degree[first_source_index])\n",
    "                    features['Nw_inDegree0'].append(in_degree[first_source_index])\n",
    "                    features['Nw_outDegree0'].append(out_degree[first_source_index])\n",
    "                    features['Nw_degree-1'].append(degree[last_source_index])\n",
    "                    features['Nw_inDegree-1'].append(in_degree[last_source_index])\n",
    "                    features['Nw_outDegree-1'].append(out_degree[last_source_index])\n",
    "                    features['Nw_degreeSeed0'].append(degree[int(first_source_row['seed_index'])])\n",
    "                    features['Nw_inDegreeSeed0'].append(in_degree[int(first_source_row['seed_index'])])\n",
    "                    features['Nw_outDegreeSeed0'].append(out_degree[int(first_source_row['seed_index'])])\n",
    "                    features['Nw_degreeSeed-1'].append(degree[int(last_source_row['seed_index'])])\n",
    "                    features['Nw_inDegreeSeed-1'].append(in_degree[int(last_source_row['seed_index'])])\n",
    "                    features['Nw_outDegreeSeed-1'].append(out_degree[int(last_source_row['seed_index'])])\n",
    "                    # SNw: Spreading Network\n",
    "                    features['SNw_nFriendsInfected'].append(len(sources))\n",
    "                    features['SNw_friendsInfectedRatio'].append(safe_division(len(sources), user_row['friends_count']))\n",
    "                    features['SNw_generation0'].append(first_source_row['generation'])\n",
    "                    features['SNw_generation-1'].append(last_source_row['generation'])\n",
    "                    features['SNw_timeSinceSeed0'].append(first_source_row['time_since_seed'])\n",
    "                    features['SNw_timeSinceSeed-1'].append(last_source_row['time_since_seed'])\n",
    "\n",
    "                    infected_dataframe = network_simulation[network_simulation.time_lapsed <= current_time]\n",
    "                    total_nodes_infected = infected_dataframe.shape[0]\n",
    "                    total_in_degree = sum(infected_dataframe.friends_count)\n",
    "                    total_out_degree = sum(infected_dataframe.followers_count)\n",
    "\n",
    "                    features['SNw_totalNodesInfected'].append(total_nodes_infected)\n",
    "                    features['SNw_nodeInfectedCentrality'].append(len(sources)/total_nodes_infected)\n",
    "                    features['SNw_totalInDegree'].append(total_in_degree)\n",
    "                    features['SNw_totalOutDegree'].append(total_out_degree)\n",
    "                    features['SNw_inDegreeCentrality'].append(in_degree[index]/total_in_degree)\n",
    "                    features['SNw_inDegreeCentrality0'].append(in_degree[first_source_index]/total_in_degree)\n",
    "                    features['SNw_inDegreeCentrality-1'].append(in_degree[last_source_index]/total_in_degree)\n",
    "                    features['SNw_outDegreeCentrality'].append(out_degree[index]/total_out_degree)\n",
    "                    features['SNw_outDegreeCentrality0'].append(out_degree[first_source_index]/total_out_degree)\n",
    "                    features['SNw_outDegreeCentrality-1'].append(out_degree[last_source_index]/total_out_degree)\n",
    "                    features['SNw_inDegreeCentralitySeed0'].append(in_degree[int(first_source_row['seed_index'])]/total_in_degree)\n",
    "                    features['SNw_outDegreeCentralitySeed0'].append(out_degree[int(first_source_row['seed_index'])]/total_out_degree)\n",
    "                    features['SNw_inDegreeCentralitySeed-1'].append(in_degree[int(last_source_row['seed_index'])]/total_in_degree)\n",
    "                    features['SNw_outDegreeCentralitySeed-1'].append(out_degree[int(last_source_row['seed_index'])]/total_out_degree)\n",
    "                    # Stat: Statistical\n",
    "                    features['Stat_average_kOut'].append(round(mean(degreeList), 1))\n",
    "                    features['Stat_average_t'].append(round(mean(timeList), 1))\n",
    "                    features['Stat_average_deltaDays'].append(sources_dataframe.user_created_days.mean())\n",
    "                    features['Stat_average_statusesCount'].append(sources_dataframe.statuses_count.mean())\n",
    "                    features['Stat_average_followersCount'].append(sources_dataframe.followers_count.mean())\n",
    "                    features['Stat_average_favouritesCount'].append(sources_dataframe.favourites_count.mean())\n",
    "                    features['Stat_average_friendsCount'].append(sources_dataframe.friends_count.mean())\n",
    "                    features['Stat_average_listedCount'].append(sources_dataframe.listed_count.mean())\n",
    "                    features['Stat_average_normalizedUserStatusesCount'].append(sources_dataframe.normalized_statuses_count.mean())\n",
    "                    features['Stat_average_normalizedUserFollowersCount'].append(sources_dataframe.normalized_followers_count.mean())\n",
    "                    features['Stat_average_normalizedUserFavouritesCount'].append(sources_dataframe.normalized_favourites_count.mean())\n",
    "                    features['Stat_average_normalizedUserListedCount'].append(sources_dataframe.normalized_listed_count.mean())\n",
    "                    features['Stat_average_normalizedUserFriendsCount'].append(sources_dataframe.normalized_friends_count.mean())\n",
    "                    features['Stat_max_kOut'].append(max(degreeList))\n",
    "                    features['Stat_min_kOut'].append(min(degreeList))\n",
    "                else:\n",
    "                    features['UsM_deltaDays0'].append(None)\n",
    "                    features['UsM_statusesCount0'].append(None)\n",
    "                    features['UsM_followersCount0'].append(None)\n",
    "                    features['UsM_favouritesCount0'].append(None)\n",
    "                    features['UsM_friendsCount0'].append(None)\n",
    "                    features['UsM_listedCount0'].append(None)\n",
    "                    features['UsM_normalizedUserStatusesCount0'].append(None)\n",
    "                    features['UsM_normalizedUserFollowersCount0'].append(None)\n",
    "                    features['UsM_normalizedUserFavouritesCount0'].append(None)\n",
    "                    features['UsM_normalizedUserListedCount0'].append(None)\n",
    "                    features['UsM_normalizedUserFriendsCount0'].append(None)\n",
    "                    features['UsM_deltaDays-1'].append(None)\n",
    "                    features['UsM_statusesCount-1'].append(None)\n",
    "                    features['UsM_followersCount-1'].append(None)\n",
    "                    features['UsM_favouritesCount-1'].append(None)\n",
    "                    features['UsM_friendsCount-1'].append(None)\n",
    "                    features['UsM_listedCount-1'].append(None)\n",
    "                    features['UsM_normalizedUserStatusesCount-1'].append(None)\n",
    "                    features['UsM_normalizedUserFollowersCount-1'].append(None)\n",
    "                    features['UsM_normalizedUserFavouritesCount-1'].append(None)\n",
    "                    features['UsM_normalizedUserListedCount-1'].append(None)\n",
    "                    features['UsM_normalizedUserFriendsCount-1'].append(None)\n",
    "                    # TwM: Tweet metadata\n",
    "                    features['TwM_t0'].append(None)\n",
    "                    features['TwM_tSeed0'].append(None)\n",
    "                    features['TwM_t-1'].append(None)\n",
    "                    features['TwM_tSeed-1'].append(None)\n",
    "                    features['TwM_tCurrent'].append(None)\n",
    "                    # Nw: Network\n",
    "                    features['Nw_degree'].append(None)\n",
    "                    features['Nw_inDegree'].append(None)\n",
    "                    features['Nw_outDegree'].append(None)\n",
    "                    features['Nw_degree0'].append(None)\n",
    "                    features['Nw_inDegree0'].append(None)\n",
    "                    features['Nw_outDegree0'].append(None)\n",
    "                    features['Nw_degree-1'].append(None)\n",
    "                    features['Nw_inDegree-1'].append(None)\n",
    "                    features['Nw_outDegree-1'].append(None)\n",
    "                    features['Nw_degreeSeed0'].append(None)\n",
    "                    features['Nw_inDegreeSeed0'].append(None)\n",
    "                    features['Nw_outDegreeSeed0'].append(None)\n",
    "                    features['Nw_degreeSeed-1'].append(None)\n",
    "                    features['Nw_inDegreeSeed-1'].append(None)\n",
    "                    features['Nw_outDegreeSeed-1'].append(None)\n",
    "                    # SNw: Spreading Network\n",
    "                    features['SNw_nFriendsInfected'].append(0)\n",
    "                    features['SNw_friendsInfectedRatio'].append(None)\n",
    "                    features['SNw_generation0'].append(None)\n",
    "                    features['SNw_generation-1'].append(None)\n",
    "                    features['SNw_timeSinceSeed0'].append(None)\n",
    "                    features['SNw_timeSinceSeed-1'].append(None)\n",
    "                    features['SNw_totalNodesInfected'].append(None)\n",
    "                    features['SNw_nodeInfectedCentrality'].append(None)\n",
    "                    features['SNw_totalInDegree'].append(None)\n",
    "                    features['SNw_totalOutDegree'].append(None)\n",
    "                    features['SNw_inDegreeCentrality'].append(None)\n",
    "                    features['SNw_inDegreeCentrality0'].append(None)\n",
    "                    features['SNw_inDegreeCentrality-1'].append(None)\n",
    "                    features['SNw_outDegreeCentrality'].append(None)\n",
    "                    features['SNw_outDegreeCentrality0'].append(None)\n",
    "                    features['SNw_outDegreeCentrality-1'].append(None)\n",
    "                    features['SNw_inDegreeCentralitySeed0'].append(None)\n",
    "                    features['SNw_outDegreeCentralitySeed0'].append(None)\n",
    "                    features['SNw_inDegreeCentralitySeed-1'].append(None)\n",
    "                    features['SNw_outDegreeCentralitySeed-1'].append(None)\n",
    "                    # Stat: Statistical\n",
    "                    features['Stat_average_kOut'].append(None)\n",
    "                    features['Stat_average_t'].append(None)\n",
    "                    features['Stat_average_deltaDays'].append(None)\n",
    "                    features['Stat_average_statusesCount'].append(None)\n",
    "                    features['Stat_average_followersCount'].append(None)\n",
    "                    features['Stat_average_favouritesCount'].append(None)\n",
    "                    features['Stat_average_friendsCount'].append(None)\n",
    "                    features['Stat_average_listedCount'].append(None)\n",
    "                    features['Stat_average_normalizedUserStatusesCount'].append(None)\n",
    "                    features['Stat_average_normalizedUserFollowersCount'].append(None)\n",
    "                    features['Stat_average_normalizedUserFavouritesCount'].append(None)\n",
    "                    features['Stat_average_normalizedUserListedCount'].append(None)\n",
    "                    features['Stat_average_normalizedUserFriendsCount'].append(None)\n",
    "                    features['Stat_max_kOut'].append(None)\n",
    "                    features['Stat_min_kOut'].append(None)\n",
    "\n",
    "                pbar.update(1)\n",
    "    else:\n",
    "         with tqdm(total=len(list(users[start_index: end_index].iterrows()))) as pbar: \n",
    "            for index, user_row in users[start_index: end_index].iterrows():\n",
    "                source_candidates = sorted(user_row['source_candidates'])\n",
    "                if user_row['time_lapsed'] <= current_time:\n",
    "\n",
    "                    features.at[index,'infected_status'] = True\n",
    "                    features.at[index,'infection_time'] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[index,'time_lapsed'] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[index,'source_index'] = user_row['source_index']\n",
    "                    network_simulation.loc[index,'seed_index'] = user_row['seed_index']\n",
    "                    network_simulation.loc[index, 'generation'] = user_row['generation']\n",
    "                    network_simulation.loc[index, 'time_since_seed'] = user_row['time_since_seed']\n",
    "                if isinstance(source_candidates, list):\n",
    "                    sources = network_simulation.loc[source_candidates]\n",
    "                    sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "    #                 if user_row['time_lapsed'] > current_time:\n",
    "    #                     sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "    #                 else:\n",
    "    #                      sources_dataframe = sources[sources['time_lapsed'] <= user_row['time_lapsed']]\n",
    "                    sources = sources_dataframe.index.tolist()\n",
    "                else:\n",
    "                    sources = []\n",
    "                #sources = [x for x in source_candidates if users.loc[x,'time_lapsed'] <= current_time]\n",
    "                #print(f'sources:{sources}')\n",
    "                if len(sources) > 0:\n",
    "\n",
    "                    # Assign the values here to save computation\n",
    "                    first_source_index = source_candidates[0]\n",
    "                    first_source_row = users.loc[first_source_index]\n",
    "                    first_source_seed_row = users.loc[first_source_row['seed_index']]\n",
    "\n",
    "                    inDegreeList = sources_dataframe.friends_count.tolist()\n",
    "                    outDegreeList = sources_dataframe.followers_count.tolist()\n",
    "                    degreeList = [x + y for x, y in zip(inDegreeList, outDegreeList)]\n",
    "\n",
    "                    s_ind = sources_dataframe.friends_count\n",
    "                    s_outd = sources_dataframe.followers_count\n",
    "                    outDegreeList = s_outd.tolist()\n",
    "                    inDegreeList = s_ind.tolist()\n",
    "                    degreeList = (s_ind + s_outd).tolist()\n",
    "\n",
    "                    #degreeList = list(users.loc[i, 'followers_count'] + users.loc[i, 'friends_count']  for i in sources)\n",
    "                    current_time_series = pd.Series([current_time] * len(sources))\n",
    "                    time_lapsed_series = sources_dataframe.time_lapsed\n",
    "                    timeList = (current_time_series - time_lapsed_series).tolist()\n",
    "\n",
    "\n",
    "                    last_source_index = sources[-1]\n",
    "                    last_source_row = network_simulation.loc[last_source_index]\n",
    "                    last_source_seed_row = network_simulation.loc[last_source_row['seed_index']]\n",
    "\n",
    "                    usr_index = index\n",
    "\n",
    "    #                 if user_row['time_lapsed'] <= current_time:\n",
    "\n",
    "    #                     features['infected_status'][-1] = True\n",
    "    #                     features['infection_time'][-1] = user_row['time_lapsed']\n",
    "\n",
    "    #                     network_simulation.loc[usr_index,'time_lapsed'] = user_row['time_lapsed']\n",
    "\n",
    "    #                     network_simulation.loc[usr_index,'source_index'] = user_row['source_index']\n",
    "    #                     network_simulation.loc[usr_index,'seed_index'] = user_row['seed_index']\n",
    "    #                     network_simulation.loc[usr_index, 'generation'] = user_row['generation']\n",
    "    #                     network_simulation.loc[usr_index, 'time_since_seed'] = user_row['time_since_seed']\n",
    "\n",
    "\n",
    "                    network_simulation.at[usr_index,'source_candidates'] = sources\n",
    "\n",
    "                    # UsM: User metadata\n",
    "\n",
    "                    features.at[index,'UsM_deltaDays0'] = first_source_row.user_created_days\n",
    "                    features.at[index,'UsM_statusesCount0'] = first_source_row.statuses_count\n",
    "                    features.at[index,'UsM_followersCount0'] = first_source_row.followers_count\n",
    "                    features.at[index,'UsM_favouritesCount0'] = first_source_row.favourites_count\n",
    "                    features.at[index,'UsM_friendsCount0'] = first_source_row.friends_count\n",
    "                    features.at[index,'UsM_listedCount0'] = first_source_row.listed_count\n",
    "                    features.at[index,'UsM_normalizedUserStatusesCount0'] = first_source_row.normalized_statuses_count\n",
    "                    features.at[index,'UsM_normalizedUserFollowersCount0'] = first_source_row.normalized_followers_count\n",
    "                    features.at[index,'UsM_normalizedUserFavouritesCount0'] = first_source_row.normalized_favourites_count\n",
    "                    features.at[index,'UsM_normalizedUserListedCount0'] = first_source_row.normalized_listed_count\n",
    "                    features.at[index,'UsM_normalizedUserFriendsCount0'] = first_source_row.normalized_friends_count\n",
    "                    features.at[index,'UsM_deltaDays-1'] = last_source_row.user_created_days\n",
    "                    features.at[index,'UsM_statusesCount-1'] = last_source_row.statuses_count\n",
    "                    features.at[index,'UsM_followersCount-1'] = last_source_row.followers_count\n",
    "                    features.at[index,'UsM_favouritesCount-1'] = last_source_row.favourites_count\n",
    "                    features.at[index,'UsM_friendsCount-1'] = last_source_row.friends_count\n",
    "                    features.at[index,'UsM_listedCount-1'] = last_source_row.listed_count\n",
    "                    features.at[index,'UsM_normalizedUserStatusesCount-1'] = last_source_row.normalized_statuses_count\n",
    "                    features.at[index,'UsM_normalizedUserFollowersCount-1'] = last_source_row.normalized_followers_count\n",
    "                    features.at[index,'UsM_normalizedUserFavouritesCount-1'] = last_source_row.normalized_favourites_count\n",
    "                    features.at[index,'UsM_normalizedUserListedCount-1'] = last_source_row.normalized_listed_count\n",
    "                    features.at[index,'UsM_normalizedUserFriendsCount-1'] = last_source_row.normalized_friends_count\n",
    "                    # TwM: Tweet metadata\n",
    "                    features.at[index,'TwM_t0'] = round(timeList[0], 1)\n",
    "                    features.at[index,'TwM_tSeed0'] = round(current_time - first_source_seed_row['time_lapsed'], 1)\n",
    "                    features.at[index,'TwM_t-1'] = round(timeList[-1], 1)\n",
    "                    features.at[index,'TwM_tSeed-1'] = round(current_time - last_source_seed_row['time_lapsed'], 1)\n",
    "                    features.at[index,'TwM_tCurrent'] = current_time\n",
    "                    # Nw: Network\n",
    "                    features.at[index,'Nw_degree'] = degree[index]\n",
    "                    features.at[index,'Nw_inDegree'] = in_degree[index]\n",
    "                    features.at[index,'Nw_outDegree'] = out_degree[index]\n",
    "                    features.at[index,'Nw_degree0'] = degree[first_source_index]\n",
    "                    features.at[index,'Nw_inDegree0'] = in_degree[first_source_index]\n",
    "                    features.at[index,'Nw_outDegree0'] = out_degree[first_source_index]\n",
    "                    features.at[index,'Nw_degree-1'] = degree[last_source_index]\n",
    "                    features.at[index,'Nw_inDegree-1'] = in_degree[last_source_index]\n",
    "                    features.at[index,'Nw_outDegree-1'] = out_degree[last_source_index]\n",
    "                    features.at[index,'Nw_degreeSeed0'] = degree[int(first_source_row['seed_index'])]\n",
    "                    features.at[index,'Nw_inDegreeSeed0'] = in_degree[int(first_source_row['seed_index'])]\n",
    "                    features.at[index,'Nw_outDegreeSeed0'] = out_degree[int(first_source_row['seed_index'])]\n",
    "                    features.at[index,'Nw_degreeSeed-1'] = degree[int(last_source_row['seed_index'])]\n",
    "                    features.at[index,'Nw_inDegreeSeed-1'] = in_degree[int(last_source_row['seed_index'])]\n",
    "                    features.at[index,'Nw_outDegreeSeed-1'] = out_degree[int(last_source_row['seed_index'])]\n",
    "                    # SNw: Spreading Network\n",
    "                    features.at[index,'SNw_nFriendsInfected'] = len(sources)\n",
    "                    features.at[index,'SNw_friendsInfectedRatio'] = safe_division(len(sources), user_row['friends_count'])\n",
    "                    features.at[index,'SNw_generation0'] = first_source_row['generation']\n",
    "                    features.at[index,'SNw_generation-1'] = last_source_row['generation']\n",
    "                    features.at[index,'SNw_timeSinceSeed0'] = first_source_row['time_since_seed']\n",
    "                    features.at[index,'SNw_timeSinceSeed-1'] = last_source_row['time_since_seed']\n",
    "\n",
    "                    infected_dataframe = network_simulation[network_simulation.time_lapsed <= current_time]\n",
    "                    total_nodes_infected = infected_dataframe.shape[0]\n",
    "                    total_in_degree = sum(infected_dataframe.friends_count)\n",
    "                    total_out_degree = sum(infected_dataframe.followers_count)\n",
    "\n",
    "                    features.at[index,'SNw_totalNodesInfected'] = total_nodes_infected\n",
    "                    features.at[index,'SNw_nodeInfectedCentrality'] = len(sources)/total_nodes_infected\n",
    "                    features.at[index,'SNw_totalInDegree'] = total_in_degree\n",
    "                    features.at[index,'SNw_totalOutDegree'] = total_out_degree\n",
    "                    features.at[index,'SNw_inDegreeCentrality'] = in_degree[index]/total_in_degree\n",
    "                    features.at[index,'SNw_inDegreeCentrality0'] = in_degree[first_source_index]/total_in_degree\n",
    "                    features.at[index,'SNw_inDegreeCentrality-1'] = in_degree[last_source_index]/total_in_degree\n",
    "                    features.at[index,'SNw_outDegreeCentrality'] = out_degree[index]/total_out_degree\n",
    "                    features.at[index,'SNw_outDegreeCentrality0'] = out_degree[first_source_index]/total_out_degree\n",
    "                    features.at[index,'SNw_outDegreeCentrality-1'] = out_degree[last_source_index]/total_out_degree\n",
    "                    features.at[index,'SNw_inDegreeCentralitySeed0'] = in_degree[int(first_source_row['seed_index'])]/total_in_degree\n",
    "                    features.at[index,'SNw_outDegreeCentralitySeed0'] = out_degree[int(first_source_row['seed_index'])]/total_out_degree\n",
    "                    features.at[index,'SNw_inDegreeCentralitySeed-1'] = in_degree[int(last_source_row['seed_index'])]/total_in_degree\n",
    "                    features.at[index,'SNw_outDegreeCentralitySeed-1'] = out_degree[int(last_source_row['seed_index'])]/total_out_degree\n",
    "                    # Stat: Statistical\n",
    "                    features.at[index,'Stat_average_kOut'] = round(mean(degreeList), 1)\n",
    "                    features.at[index,'Stat_average_t'] = round(mean(timeList), 1)\n",
    "                    features.at[index,'Stat_average_deltaDays'] = sources_dataframe.user_created_days.mean()\n",
    "                    features.at[index,'Stat_average_statusesCount'] = sources_dataframe.statuses_count.mean()\n",
    "                    features.at[index,'Stat_average_followersCount'] = sources_dataframe.followers_count.mean()\n",
    "                    features.at[index,'Stat_average_favouritesCount'] = sources_dataframe.favourites_count.mean()\n",
    "                    features.at[index,'Stat_average_friendsCount'] = sources_dataframe.friends_count.mean()\n",
    "                    features.at[index,'Stat_average_listedCount'] = sources_dataframe.listed_count.mean()\n",
    "                    features.at[index,'Stat_average_normalizedUserStatusesCount'] = sources_dataframe.normalized_statuses_count.mean()\n",
    "                    features.at[index,'Stat_average_normalizedUserFollowersCount'] = sources_dataframe.normalized_followers_count.mean()\n",
    "                    features.at[index,'Stat_average_normalizedUserFavouritesCount'] = sources_dataframe.normalized_favourites_count.mean()\n",
    "                    features.at[index,'Stat_average_normalizedUserListedCount'] = sources_dataframe.normalized_listed_count.mean()\n",
    "                    features.at[index,'Stat_average_normalizedUserFriendsCount'] = sources_dataframe.normalized_friends_count.mean()\n",
    "                    features.at[index,'Stat_max_kOut'] = max(degreeList)\n",
    "                    features.at[index,'Stat_min_kOut'] = min(degreeList)\n",
    "                else:\n",
    "                    features.at[index,'UsM_deltaDays0']=None\n",
    "                    features.at[index,'UsM_statusesCount0']=None\n",
    "                    features.at[index,'UsM_followersCount0']=None\n",
    "                    features.at[index,'UsM_favouritesCount0']=None\n",
    "                    features.at[index,'UsM_friendsCount0']=None\n",
    "                    features.at[index,'UsM_listedCount0']=None\n",
    "                    features.at[index,'UsM_normalizedUserStatusesCount0']=None\n",
    "                    features.at[index,'UsM_normalizedUserFollowersCount0']=None\n",
    "                    features.at[index,'UsM_normalizedUserFavouritesCount0']=None\n",
    "                    features.at[index,'UsM_normalizedUserListedCount0']=None\n",
    "                    features.at[index,'UsM_normalizedUserFriendsCount0']=None\n",
    "                    features.at[index,'UsM_deltaDays-1']=None\n",
    "                    features.at[index,'UsM_statusesCount-1']=None\n",
    "                    features.at[index,'UsM_followersCount-1']=None\n",
    "                    features.at[index,'UsM_favouritesCount-1']=None\n",
    "                    features.at[index,'UsM_friendsCount-1']=None\n",
    "                    features.at[index,'UsM_listedCount-1']=None\n",
    "                    features.at[index,'UsM_normalizedUserStatusesCount-1']=None\n",
    "                    features.at[index,'UsM_normalizedUserFollowersCount-1']=None\n",
    "                    features.at[index,'UsM_normalizedUserFavouritesCount-1']=None\n",
    "                    features.at[index,'UsM_normalizedUserListedCount-1']=None\n",
    "                    features.at[index,'UsM_normalizedUserFriendsCount-1']=None\n",
    "                    # TwM: Tweet metadata\n",
    "                    features.at[index,'TwM_t0']=None\n",
    "                    features.at[index,'TwM_tSeed0']=None\n",
    "                    features.at[index,'TwM_t-1']=None\n",
    "                    features.at[index,'TwM_tSeed-1']=None\n",
    "                    features.at[index,'TwM_tCurrent']=None\n",
    "                    # Nw: Network\n",
    "                    features.at[index,'Nw_degree']=None\n",
    "                    features.at[index,'Nw_inDegree']=None\n",
    "                    features.at[index,'Nw_outDegree']=None\n",
    "                    features.at[index,'Nw_degree0']=None\n",
    "                    features.at[index,'Nw_inDegree0']=None\n",
    "                    features.at[index,'Nw_outDegree0']=None\n",
    "                    features.at[index,'Nw_degree-1']=None\n",
    "                    features.at[index,'Nw_inDegree-1']=None\n",
    "                    features.at[index,'Nw_outDegree-1']=None\n",
    "                    features.at[index,'Nw_degreeSeed0']=None\n",
    "                    features.at[index,'Nw_inDegreeSeed0']=None\n",
    "                    features.at[index,'Nw_outDegreeSeed0']=None\n",
    "                    features.at[index,'Nw_degreeSeed-1']=None\n",
    "                    features.at[index,'Nw_inDegreeSeed-1']=None\n",
    "                    features.at[index,'Nw_outDegreeSeed-1']=None\n",
    "                    # SNw: Spreading Network\n",
    "                    features.at[index,'SNw_nFriendsInfected']=0\n",
    "                    features.at[index,'SNw_friendsInfectedRatio']=None\n",
    "                    features.at[index,'SNw_generation0']=None\n",
    "                    features.at[index,'SNw_generation-1']=None\n",
    "                    features.at[index,'SNw_timeSinceSeed0']=None\n",
    "                    features.at[index,'SNw_timeSinceSeed-1']=None\n",
    "                    features.at[index,'SNw_totalNodesInfected']=None\n",
    "                    features.at[index,'SNw_nodeInfectedCentrality']=None\n",
    "                    features.at[index,'SNw_totalInDegree']=None\n",
    "                    features.at[index,'SNw_totalOutDegree']=None\n",
    "                    features.at[index,'SNw_inDegreeCentrality']=None\n",
    "                    features.at[index,'SNw_inDegreeCentrality0']=None\n",
    "                    features.at[index,'SNw_inDegreeCentrality-1']=None\n",
    "                    features.at[index,'SNw_outDegreeCentrality']=None\n",
    "                    features.at[index,'SNw_outDegreeCentrality0']=None\n",
    "                    features.at[index,'SNw_outDegreeCentrality-1']=None\n",
    "                    features.at[index,'SNw_inDegreeCentralitySeed0']=None\n",
    "                    features.at[index,'SNw_outDegreeCentralitySeed0']=None\n",
    "                    features.at[index,'SNw_inDegreeCentralitySeed-1']=None\n",
    "                    features.at[index,'SNw_outDegreeCentralitySeed-1']=None\n",
    "                    # Stat: Statistical\n",
    "                    features.at[index,'Stat_average_kOut']=None\n",
    "                    features.at[index,'Stat_average_t']=None\n",
    "                    features.at[index,'Stat_average_deltaDays']=None\n",
    "                    features.at[index,'Stat_average_statusesCount']=None\n",
    "                    features.at[index,'Stat_average_followersCount']=None\n",
    "                    features.at[index,'Stat_average_favouritesCount']=None\n",
    "                    features.at[index,'Stat_average_friendsCount']=None\n",
    "                    features.at[index,'Stat_average_listedCount']=None\n",
    "                    features.at[index,'Stat_average_normalizedUserStatusesCount']=None\n",
    "                    features.at[index,'Stat_average_normalizedUserFollowersCount']=None\n",
    "                    features.at[index,'Stat_average_normalizedUserFavouritesCount']=None\n",
    "                    features.at[index,'Stat_average_normalizedUserListedCount']=None\n",
    "                    features.at[index,'Stat_average_normalizedUserFriendsCount']=None\n",
    "                    features.at[index,'Stat_max_kOut']=None\n",
    "                    features.at[index,'Stat_min_kOut']=None   \n",
    "                pbar.update(1)          \n",
    "    processed_dataframe = pd.DataFrame(features)\n",
    "    return processed_dataframe, network_simulation\n",
    "\n",
    "\n",
    "def data_preparation_process(path_of_user_data, path_of_follower_list=None, path_of_friend_list=None, feature_file_path=None, network_file_path=None, end_min=360):\n",
    "    \n",
    "    users = load_pickle_file(path_of_user_data)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "    unique_users = set(users.index)\n",
    "#     print(users['source_candidates'])\n",
    "    users[\"source_candidates\"] = users[\"source_candidates\"].map(lambda x: filter_id_list(unique_users, x))\n",
    "\n",
    "    network_simulation = network_simulation_init(users, end_min) # initialise network_simulation \n",
    "    start_index = 0\n",
    "    end_index = len(users)\n",
    "    features, network_simulation = construct_features(users, network_simulation, start_index, end_index, end_min)\n",
    "    network_simulation=network_simulation_merge_friends_list(network_simulation, path_of_friend_list)\n",
    "    network_simulation=network_simulation_merge_followers_list(network_simulation, path_of_follower_list)\n",
    "#     print(network_simulation)\n",
    "#     print(network_simulation['source_candidates'])\n",
    "#     print(network_simulation.at[1000,'source_candidates'])\n",
    "#     print(network_simulation[network_simulation['time_lapsed'].isnull() == False].index.values)\n",
    "#     print(list(features['infected_status']))\n",
    "#     print(list(features['infected_status']).count(1))\n",
    "#     print(list(network_simulation[]))\n",
    "    save_pickle_file(feature_file_path,features)\n",
    "    save_pickle_file(network_file_path,network_simulation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\users.dat\n",
      "Loaded 5973 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5973/5973 [00:44<00:00, 134.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\followers.dat\n",
      "Loaded 5011 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bird\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\friends.dat\n",
      "Loaded 5011 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bird\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 3460 3461 3462]\n",
      "3463\n",
      "Dumping data to path F:\\twitter_data\\givenchy\\pickle\\feature_6_hour.pkl\n",
      "Finished dumping data to path F:\\twitter_data\\givenchy\\pickle\\feature_6_hour.pkl\n",
      "Dumping data to path F:\\twitter_data\\givenchy\\pickle\\network_simulation_6_hour.pkl\n",
      "Finished dumping data to path F:\\twitter_data\\givenchy\\pickle\\network_simulation_6_hour.pkl\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# data_1 = [[10, 20, 30, 40, 50], [11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35], [41, 42, 43, 44, 45]]\n",
    "# # date_range1 = pd.date_range(start=\"20190301\", periods=5)\n",
    "# df1 = pd.DataFrame(data=data_1, \n",
    "#                   columns=['A', 'B', 'C', 'D', 'E'])\n",
    "# print(df1)\n",
    "# def tet(df):\n",
    "#     df['newco']=''\n",
    "# tet(df1)\n",
    "# print(df1)\n",
    "user_data = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "follower_list='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\friends.dat'\n",
    "friend_list='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\followers.dat'\n",
    "feature_file_path='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "network_file_path='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl'\n",
    "simulation_start_min= 360\n",
    "    \n",
    "data_preparation_process(user_data, follower_list, friend_list, feature_file_path, network_file_path, end_min=simulation_start_min)\n",
    "\n",
    "# initial_dataset = load_pickle_file('F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl')\n",
    "# users = load_pickle_file(user_data)  \n",
    "# users.reset_index(drop =True , inplace =True)\n",
    "# print(users.at[3462, 'time_lapsed'])\n",
    "# for i in range(0,5973):\n",
    "#     print(i,users.at[i, 'time_lapsed'])\n",
    "# for i in range(0,5973):\n",
    "#     print(i,initial_dataset.at[i, 'source_candidates'])\n",
    "# print(initial_dataset)\n",
    "# print(initial_dataset.at[2811, 'followers_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\network_simulation_6_hour.pkl\n",
      "Loaded 5973 entries\n",
      "[1591, 3587, 20, 3810, 1807, 577, 3390, 1012, 1656, 0, 1953, 3203]\n"
     ]
    }
   ],
   "source": [
    "initial_dataset = load_pickle_file('F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl')\n",
    "# features = load_pickle_file('F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl')\n",
    "# print(features)\n",
    "# for i in range(0,5973):\n",
    "#     print(i,features.at[i, 'Stat_min_kOut'])\n",
    "def testfun(initial_dataset):\n",
    "    followers_indices = list(set(initial_dataset['id']).intersection(set(initial_dataset.loc[5].followers_list)))\n",
    "    followers_indices = [list(initial_dataset['id']).index(x) for x in followers_indices]     \n",
    "    print(followers_indices)\n",
    "testfun(initial_dataset)\n",
    "# for i in range(0,5973):\n",
    "#     print(i,initial_dataset.at[i, 'time_lapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def prepare_training_data(initial_features):\n",
    "    df = initial_features\n",
    "    \n",
    "    df['label']=df['infected_status'].apply(lambda x: 1 if x == True else 0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.drop(columns = ['user_id', 'infected_status', 'infection_time', 'followers_list'],axis = 1)\n",
    "    \n",
    "    # Converting all type to float, to prepare for feature selection\n",
    "    df = df.astype('float')\n",
    "    # Reset index, with drop equals to true to avoid setting old index as a new column\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Visualize distribution\n",
    "    print('[Original] data counts, with uninfected (0): {}, infected (1): {}'.format(\n",
    "        df['label'].value_counts()[0],\n",
    "        df['label'].value_counts()[1]\n",
    "    ))\n",
    "#     df.groupby(['TwM_tCurrent','label']).size().unstack(fill_value=0).plot.bar(title='Original Data Distribution')\n",
    "    \n",
    "    columns = list(df.columns)\n",
    "    columns.remove('label')\n",
    "    \n",
    "    X = df[columns]\n",
    "    y = df[['label']]\n",
    "    return df, X, y\n",
    "\n",
    "def upsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.label==0] # Uninfected is the major class\n",
    "    df_minority = df[df.label==1] # Infected is the minor class\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=len(df_majority),    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # Display new class counts\n",
    "    print(df_upsampled.label.value_counts())\n",
    "    \n",
    "    return df_upsampled\n",
    "\n",
    "\n",
    "def downsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.label==0] # Uninfected is the major class\n",
    "    df_minority = df[df.label==1] # Infected is the minor class\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=len(df_minority),     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    # Display new class counts\n",
    "    print(df_downsampled.label.value_counts())\n",
    "    \n",
    "    return df_downsampled\n",
    "\n",
    "def blcsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_minority = df[df.label==0] # Uninfected is the major class  small  2600\n",
    "    df_majority = df[df.label==1] # Infected is the minor class     big    170\n",
    "    df_blcmpled = None\n",
    "    if len(df_majority) > len(df_minority):\n",
    "        df_blcmpled = upsample(df)\n",
    "    else:\n",
    "        df_blcmpled = downsample(df)\n",
    "\n",
    "#         df_majority = df[df.label==0] # Uninfected is the major class  small  2600\n",
    "#         df_minority = df[df.label==1] # Infected is the minor class     big   170\n",
    "    \n",
    "\n",
    "#     # Upsample minority class\n",
    "#     df_majority_downsampled = resample(df_majority, \n",
    "#                                      replace=False,    # sample without replacement\n",
    "#                                      n_samples=len(df_minority),     # to match minority class\n",
    "#                                      random_state=123) # reproducible results\n",
    "\n",
    "#     # Combine minority class with downsampled majority class\n",
    "#     df_blcmpled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "#     # Display new class counts\n",
    "#     print(df_blcmpled.label.value_counts())\n",
    "    \n",
    "    return df_blcmpled\n",
    "\n",
    "\n",
    "def data_training_process(user_data_path, initial_features_path, model_save_path, rebalance_method = 'blc', start_hour=6, model=xgb, param=None):\n",
    "    initial_features = load_pickle_file(initial_features_path)\n",
    "    users = load_pickle_file(user_data_path)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "\n",
    "    df, X, y = prepare_training_data(initial_features)\n",
    "    feature_columns = X.columns\n",
    "    print('There are {} Features'.format(len(feature_columns)))\n",
    "    #xgboost\n",
    "    if param == None:\n",
    "        param = {\n",
    "            'max_depth':3,\n",
    "            # Step size shrinkage used in update to prevents overfitting. \n",
    "            # After each boosting step, we can directly get the weights of new features, \n",
    "            # and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "            'eta': 0.1,\n",
    "            # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "            # The larger gamma is, the more conservative the algorithm will be.\n",
    "            'gamma':10,\n",
    "            # Minimum sum of instance weight (hessian) needed in a child. \n",
    "            # If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "            # then the building process will give up further partitioning.\n",
    "            # The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "            'min_child_weight':10,\n",
    "            'silent': 1, # 0 means printing running messages, 1 means silent mode\n",
    "            'objective': 'binary:logistic',\n",
    "            'subsample': 0.9\n",
    "        }\n",
    "    param['nthread'] = cpu_count\n",
    "    param['eval_metric'] = ['auc']\n",
    "    num_boost_round = 100\n",
    "\n",
    "#     xgb_model = train(df, X, y, model, param, 2, num_boost_round, rebalance_method)\n",
    "    \n",
    "  \n",
    "   \n",
    "    columns = list(df.columns)\n",
    "    columns.remove('label')\n",
    "    if rebalance_method == 'up':\n",
    "        df_rebalance = upsample(df)\n",
    "    if rebalance_method == 'down':\n",
    "        df_rebalance = downsample(df)\n",
    "    if rebalance_method == 'blc':\n",
    "        df_rebalance = blcsample(df)\n",
    "    X = df_rebalance[columns]\n",
    "    y = df_rebalance[['label']]\n",
    "    print(\"---There are \"+str(len(X))+\" points of data for training.---\")\n",
    "\n",
    "    # train XGBoost model\n",
    "\n",
    "    trained_model = model.train(param, model.DMatrix(X, label=y), num_boost_round)\n",
    "    X = model.DMatrix(X)\n",
    "    res = trained_model.predict(X)\n",
    "    y = list(y['label'])\n",
    "\n",
    "    err = 0\n",
    "    for i in range (0, len(res)):\n",
    "        if res[i]>0.5 and y[i]!=1:\n",
    "                err+=1\n",
    "        elif res[i]<=0.5 and y[i]!=0:\n",
    "                err+=1\n",
    "\n",
    "    print(\"training accuracy: \"+str(1-err/len(res)))\n",
    "    \n",
    "    with open(model_save_path, 'wb') as file:\n",
    "        pickle.dump(trained_model, file)\n",
    "\n",
    "\n",
    "    # explain the model's predictions using SHAP values\n",
    "    # (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "    #explainer = shap.TreeExplainer(model)\n",
    "    #shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # visualize the first prediction's explanation\n",
    "    #shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\feature_6_hour.pkl\n",
      "Loaded 5973 entries\n",
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\users.dat\n",
      "Loaded 5973 entries\n",
      "[Original] data counts, with uninfected (0): 2510, infected (1): 3463\n",
      "There are 88 Features\n",
      "1.0    2510\n",
      "0.0    2510\n",
      "Name: label, dtype: int64\n",
      "---There are 5020 points of data for training.---\n",
      "training accuracy: 0.7173306772908367\n"
     ]
    }
   ],
   "source": [
    "initial_features_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "user_data_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "model_save_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\trained_model.dat'\n",
    "model = xgb\n",
    "data_training_process(user_data_path, initial_features_path, model_save_path, rebalance_method = 'blc')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_features(source_id,target_id,features,network_simulation,current_time):\n",
    "    \n",
    "    in_degree = list(network_simulation.friends_count)\n",
    "    out_degree = list(network_simulation.followers_count)\n",
    "    degree = in_degree + out_degree\n",
    "    \n",
    "    if isinstance(network_simulation.loc[target_id,'source_candidates'],list) and len(network_simulation.loc[target_id,'source_candidates']) > 0 :\n",
    "        \n",
    "        source_candidates = sorted(network_simulation.loc[target_id,'source_candidates'])\n",
    "        nf = network_simulation.loc[source_candidates]\n",
    "        \n",
    "        sources = nf[nf['time_lapsed'] <= current_time].index.tolist()\n",
    "        \n",
    "        if len(sources) > 0:\n",
    "        \n",
    "            first_source_index = source_candidates[0]\n",
    "            first_source_row = network_simulation.loc[first_source_index]\n",
    "            first_source_seed_row = network_simulation.loc[first_source_row['seed_index']]\n",
    "\n",
    "            sources_dataframe = network_simulation.loc[sources]\n",
    "            degreeList = list(degree[i] for i in sources)\n",
    "            inDegreeList = list(in_degree[i] for i in sources)\n",
    "            outDegreeList = list(out_degree[i] for i in sources)\n",
    "            degreeList = list(network_simulation.loc[i, 'followers_count'] + network_simulation.loc[i, 'friends_count']  for i in sources)\n",
    "            timeList = [current_time - network_simulation.loc[x,'time_lapsed'] for x in sources]\n",
    "\n",
    "\n",
    "            last_source_index = sources[-1]    \n",
    "            try:\n",
    "                last_source_row = network_simulation.loc[last_source_index]\n",
    "                last_source_seed_row = network_simulation.loc[last_source_row['seed_index']]\n",
    "            except:\n",
    "                print(f\"target_index:{target_id}, last_source_row['seed_index'] : {last_source_row['seed_index']}\")\n",
    "                print(f\"last_source_index:{last_source_index}\")\n",
    "\n",
    "\n",
    "            #Extraction\n",
    "            #Columns which are added for simulation, but they are not used as features for model prediction\n",
    "\n",
    "            user_row = network_simulation.loc[target_id]\n",
    "\n",
    "\n",
    "            # UsM: User metadata                    \n",
    "\n",
    "            features.loc[target_id,'UsM_deltaDays0'] = first_source_row.user_created_days\n",
    "            features.loc[target_id,'UsM_statusesCount0'] = first_source_row.statuses_count\n",
    "            features.loc[target_id,'UsM_followersCount0'] = first_source_row.followers_count\n",
    "            features.loc[target_id,'UsM_favouritesCount0'] = first_source_row.favourites_count\n",
    "            features.loc[target_id,'UsM_friendsCount0'] = first_source_row.friends_count\n",
    "            features.loc[target_id,'UsM_listedCount0'] = first_source_row.listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserStatusesCount0'] = first_source_row.normalized_statuses_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFollowersCount0'] = first_source_row.normalized_followers_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFavouritesCount0'] = first_source_row.normalized_favourites_count\n",
    "            features.loc[target_id,'UsM_normalizedUserListedCount0'] = first_source_row.normalized_listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFriendsCount0'] = first_source_row.normalized_friends_count\n",
    "            features.loc[target_id,'UsM_deltaDays-1'] = last_source_row.user_created_days\n",
    "            features.loc[target_id,'UsM_statusesCount-1'] = last_source_row.statuses_count\n",
    "            features.loc[target_id,'UsM_followersCount-1'] = last_source_row.followers_count\n",
    "            features.loc[target_id,'UsM_favouritesCount-1'] = last_source_row.favourites_count\n",
    "            features.loc[target_id,'UsM_friendsCount-1'] = last_source_row.friends_count\n",
    "            features.loc[target_id,'UsM_listedCount-1'] = last_source_row.listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserStatusesCount-1'] = last_source_row.normalized_statuses_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFollowersCount-1'] = last_source_row.normalized_followers_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFavouritesCount-1'] = last_source_row.normalized_favourites_count\n",
    "            features.loc[target_id,'UsM_normalizedUserListedCount-1'] = last_source_row.normalized_listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFriendsCount-1'] = last_source_row.normalized_friends_count\n",
    "            # TwM: Tweet metadata\n",
    "            features.loc[target_id,'TwM_t0'] = round(timeList[0], 1)\n",
    "            features.loc[target_id,'TwM_tSeed0'] = round(current_time - first_source_seed_row['time_lapsed'], 1)\n",
    "            features.loc[target_id,'TwM_t-1'] = round(timeList[-1], 1)\n",
    "            features.loc[target_id,'TwM_tSeed-1'] = round(current_time - last_source_seed_row['time_lapsed'], 1)\n",
    "            features.loc[target_id,'TwM_tCurrent'] = current_time\n",
    "            # Nw: Network\n",
    "            features.loc[target_id,'Nw_degree'] = degree[target_id]\n",
    "            features.loc[target_id,'Nw_inDegree'] = in_degree[target_id]\n",
    "            features.loc[target_id,'Nw_outDegree'] = out_degree[target_id]\n",
    "            features.loc[target_id,'Nw_degree0'] = degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_inDegree0'] = in_degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_outDegree0'] = out_degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_degree-1'] = degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_inDegree-1'] = in_degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_outDegree-1'] = out_degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_degreeSeed0'] = degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_inDegreeSeed0'] = in_degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_outDegreeSeed0'] = out_degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_degreeSeed-1'] = degree[int(last_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_inDegreeSeed-1'] = in_degree[int(last_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_outDegreeSeed-1'] = out_degree[int(last_source_row['seed_index'])]\n",
    "            # SNw: Spreading Network\n",
    "            features.loc[target_id,'SNw_nFriendsInfected'] = len(sources)\n",
    "            features.loc[target_id,'SNw_friendsInfectedRatio'] = safe_division(len(sources), user_row['friends_count'])\n",
    "            features.loc[target_id,'SNw_generation0'] = first_source_row['generation']\n",
    "            features.loc[target_id,'SNw_generation-1'] = last_source_row['generation']\n",
    "            features.loc[target_id,'SNw_timeSinceSeed0'] = first_source_row['time_since_seed']\n",
    "            features.loc[target_id,'SNw_timeSinceSeed-1'] = last_source_row['time_since_seed']\n",
    "\n",
    "            infected_dataframe = network_simulation[network_simulation.time_lapsed <= current_time]\n",
    "            total_nodes_infected = infected_dataframe.shape[0]\n",
    "            total_in_degree = sum(infected_dataframe.friends_count)\n",
    "            total_out_degree = sum(infected_dataframe.followers_count)\n",
    "\n",
    "            features.loc[target_id,'SNw_totalNodesInfected'] = total_nodes_infected\n",
    "            features.loc[target_id,'SNw_nodeInfectedCentrality'] = len(sources)/total_nodes_infected\n",
    "            features.loc[target_id,'SNw_totalInDegree'] = total_in_degree\n",
    "            features.loc[target_id,'SNw_totalOutDegree'] = total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality'] = in_degree[target_id]/total_in_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality0'] = in_degree[first_source_index]/total_in_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality-1'] = in_degree[last_source_index]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality'] = out_degree[target_id]/total_out_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality0'] = out_degree[first_source_index]/total_out_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality-1'] = out_degree[last_source_index]/total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentralitySeed0'] = in_degree[int(first_source_row['seed_index'])]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentralitySeed0'] = out_degree[int(first_source_row['seed_index'])]/total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentralitySeed-1'] = in_degree[int(last_source_row['seed_index'])]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentralitySeed-1'] = out_degree[int(last_source_row['seed_index'])]/total_out_degree\n",
    "            # Stat: Statistical\n",
    "            features.loc[target_id,'Stat_average_kOut'] = round(mean(degreeList), 1)\n",
    "            features.loc[target_id,'Stat_average_t'] = round(mean(timeList), 1)\n",
    "            features.loc[target_id,'Stat_average_deltaDays'] = sources_dataframe.user_created_days.mean()\n",
    "            features.loc[target_id,'Stat_average_statusesCount'] = sources_dataframe.statuses_count.mean()\n",
    "            features.loc[target_id,'Stat_average_followersCount'] = sources_dataframe.followers_count.mean()\n",
    "            features.loc[target_id,'Stat_average_favouritesCount'] = sources_dataframe.favourites_count.mean()\n",
    "            features.loc[target_id,'Stat_average_friendsCount'] = sources_dataframe.friends_count.mean()\n",
    "            features.loc[target_id,'Stat_average_listedCount'] = sources_dataframe.listed_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserStatusesCount'] = sources_dataframe.normalized_statuses_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFollowersCount'] = sources_dataframe.normalized_followers_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFavouritesCount'] = sources_dataframe.normalized_favourites_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserListedCount'] = sources_dataframe.normalized_listed_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFriendsCount'] = sources_dataframe.normalized_friends_count.mean()\n",
    "            features.loc[target_id,'Stat_max_kOut'] = max(degreeList)\n",
    "            features.loc[target_id,'Stat_min_kOut'] = min(degreeList)\n",
    "    #processed_dataframe = pd.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def simulation(features,initial_dataset,network_simulation,current_time,model,infected_record):\n",
    "    predictedt_infected_list = []\n",
    "    infected_users_indices = network_simulation[network_simulation['time_lapsed'].isnull() == False].index.values\n",
    "    print(\"there are \"+ str(len(infected_users_indices)) +\" infected users.\")\n",
    "    count = 0\n",
    "    for i in infected_users_indices:\n",
    "        if(count%1000==0):\n",
    "            print(\"=====progress: \"+str(count)+\"/\"+str(len(infected_users_indices))+\" users=====\")\n",
    "        count+=1\n",
    "        if isinstance(network_simulation.loc[i,'followers_list'],list):\n",
    "   \n",
    "            followers_ids = list(set(network_simulation['id']).intersection(set(network_simulation.loc[i].followers_list)))\n",
    "   \n",
    "            followers_indices = [list(network_simulation['id']).index(x) for x in followers_ids]      \n",
    "#             followers_indices = [network_simulation[network_simulation['id'] == x].index.values.item() for x in network_simulation.loc[i].followers_list]\n",
    "#             print(followers_indices)\n",
    "            uninfected_followers_indices = [y for y in followers_indices if np.isnan(network_simulation.loc[y, 'time_lapsed']) == True]\n",
    "\n",
    "            if len(uninfected_followers_indices) > 0:\n",
    "                for j in uninfected_followers_indices:\n",
    "                    #print(f\"j:{j}\")\n",
    "                    source_index = i\n",
    "                    target_index = j\n",
    "                    processed_dataframe = update_features(source_index,target_index,features,network_simulation,current_time)\n",
    "#                     print(f\"len processed_dataframe: {len(processed_dataframe)}\")\n",
    "                    try:\n",
    "                        valid_row = processed_dataframe.loc[[target_index]]\n",
    "                    except:\n",
    "                        print(f\"source_index:{source_index}\")\n",
    "                        print(f\"target_index:{target_index}\")\n",
    "                        processed_dataframe.to_csv(path+'processed_dataframe.csv')\n",
    "                        print(processed_dataframe.loc[[target_index]])\n",
    "#                     print(f\"(valid_row.columns):{list(valid_row.columns.values)}\")\n",
    "                    \n",
    "#                     for i in range(valid_row.columns):\n",
    "#                         print(f\"valid_row:{i}\")\n",
    "                    #valid = valid_row.drop(['user_id','infected_status','infection_time','followers_list','Nw_inDegree','Nw_outDegree'],axis=1)\n",
    "                    valid = valid_row.drop(['user_id','infected_status','infection_time','followers_list'],axis=1)\n",
    "\n",
    "                    valid = valid.astype('float64')\n",
    "\n",
    "                    columns = list(valid.columns)\n",
    "                    X = valid[columns]\n",
    "                    \n",
    "                    pre_data = xgb.DMatrix(X)\n",
    "                    #print(f\"pre_data.columns:{pre_data}\")\n",
    "                    infec = model.predict(pre_data)\n",
    "                    if infec > 0.4:\n",
    "                        infected_record.append(j)\n",
    "                        predictedt_infected_list.append(j)\n",
    "                        print(\"user \"+str(j)+\" Infected\")\n",
    "                        network_simulation.loc[target_index,'time_lapsed'] = current_time\n",
    "                        network_simulation.loc[target_index,'source_index'] = source_index  #why source_index not first_source_index\n",
    "                        network_simulation.loc[target_index,'seed_index'] = network_simulation.loc[source_index,'seed_index']\n",
    "                        network_simulation.loc[target_index,'generation'] = network_simulation.loc[source_index,'generation'] + 1\n",
    "                        seed_index = network_simulation.loc[target_index,'seed_index']\n",
    "                        network_simulation.loc[target_index,'time_since_seed'] = current_time - network_simulation.loc[seed_index,'time_lapsed']\n",
    "                        followers_of_node = network_simulation.loc[target_index,'followers_list'] \n",
    "                        if isinstance(followers_of_node,list):\n",
    "                            for f in followers_of_node:\n",
    "                                if np.isnan(network_simulation[network_simulation['id'] == f]['time_lapsed'].values):\n",
    "                                    follower_index = network_simulation[network_simulation['id'] == f].index.values\n",
    "                                    try:\n",
    "                                        list(network_simulation.loc[follower_index,'source_candidates'].values).append(f)\n",
    "                                    except:\n",
    "                                        print(f\"source_candidates:{network_simulation.loc[follower_index,'source_candidates'].values}\")\n",
    "                                        print(f\"f:{f}\")\n",
    "    \n",
    "    return network_simulation, predictedt_infected_list\n",
    "\n",
    "\n",
    "\n",
    "def incremental_trained_model(start_index, users, initial_features, current_model, params=None, num_boost_round=100):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'max_depth':3,\n",
    "            'eta': 0.1,\n",
    "            'gamma':10,\n",
    "            'min_child_weight':10,\n",
    "            'silent': 1, # 0 means printing running messages, 1 means silent mode\n",
    "            'objective': 'binary:logistic',\n",
    "            'subsample': 0.9\n",
    "        }\n",
    "    params['nthread'] = cpu_count\n",
    "    params['eval_metric'] = ['auc']\n",
    "    num_boost_round = 100\n",
    "    rebalance_method = 'blc'\n",
    "    \n",
    "    initial_features = initial_features.loc[start_index:]\n",
    "    df, X, y = prepare_training_data(initial_features)\n",
    "\n",
    "    feature_columns = X.columns\n",
    "    \n",
    "    columns = list(df.columns)\n",
    "    columns.remove('label')\n",
    "    if rebalance_method == 'up':\n",
    "        df_rebalance = upsample(df)\n",
    "    if rebalance_method == 'down':\n",
    "        df_rebalance = downsample(df)\n",
    "    if rebalance_method == 'blc':\n",
    "        df_rebalance = blcsample(df)\n",
    "        \n",
    "    X = df_rebalance[columns]\n",
    "    y = df_rebalance[['label']]\n",
    "    \n",
    "    new_model = xgb.train(params, xgb.DMatrix(X, label=y), num_boost_round, xgb_model=current_model)\n",
    "#     X = xgb.DMatrix(X)\n",
    "#     res = model.predict(X)\n",
    "    \n",
    "    return new_model\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def simulation_process(current_time, total_time_duration, interval, features_path, network_path, user_path, model_path, model_params, simulation_result_path):\n",
    "    initial_features = load_pickle_file(features_path)\n",
    "    initial_dataset = load_pickle_file(network_path)\n",
    "    users = load_pickle_file(user_path)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "\n",
    "    model_infile = open(model_path,'rb')\n",
    "    model = pickle.load(model_infile)\n",
    "    model_infile.close()\n",
    "#     total_time_duration = 540\n",
    "#     interval = 30\n",
    "    #current_time = 360\n",
    "#     current_time = 360\n",
    "    #current_time = 480\n",
    "    #current_time = 540\n",
    "    #current_time = 720\n",
    "\n",
    "#     time 360 - 390 :  3463-3609\n",
    "#       time 390 - 420 :   3610 - 3710\n",
    "    features = initial_features\n",
    "    network_simulation = initial_dataset\n",
    "    id_set = set(network_simulation['id'].tolist())\n",
    "\n",
    "    print(\"Simulation started\")\n",
    "    start_time = time.time()\n",
    "# test\n",
    "    infected_record=[]\n",
    "    ini_start_index = len(initial_dataset[initial_dataset['time_lapsed'].isnull() == False].index.values)\n",
    "    end_index = 0\n",
    "    while current_time < total_time_duration:\n",
    "        start_index = len(initial_dataset[initial_dataset['time_lapsed'].isnull() == False].index.values)\n",
    "        print(f\"current_time:{current_time}\")\n",
    "        print(\"=================================================\")\n",
    "        #network_simulation = simulation(features,initial_dataset,network_simulation,current_time)\n",
    "        this_timestep_network_simulation, predicted_infected_list = simulation(features,initial_dataset,network_simulation,current_time, model, infected_record)\n",
    "        #print(f\"current_time:{current_time}\")\n",
    "        next_time_step = current_time+interval\n",
    "        \n",
    "#   ============================\n",
    "        initial_dataset = network_simulation_init(users, next_time_step)\n",
    "        end_index = len(initial_dataset[initial_dataset['time_lapsed'].isnull() == False].index.values)-1\n",
    "        true_predicted = 0\n",
    "        for each in predicted_infected_list:\n",
    "            if each > start_index and each< end_index:\n",
    "                true_predicted+=1\n",
    "        precision = safe_division(true_predicted, len(predicted_infected_list)) \n",
    "        recall = safe_division( true_predicted, end_index - start_index)\n",
    "        F1 = safe_division(2.0*precision*recall, precision+recall)\n",
    "        print('++++++++At simulation period '+str(current_time)+' to '+str(next_time_step)+'++++++++')\n",
    "        print(str(end_index - start_index)+' users infected in reality')\n",
    "        print('total positive prediction:  '+str(len(predicted_infected_list)))\n",
    "        print('total true posisive: '+str(true_predicted))\n",
    "        print('total false posisive: '+str(len(predicted_infected_list)-true_predicted))\n",
    "        print('precision: '+str( precision ))\n",
    "        print('recall: '+str(recall))\n",
    "        print('F1: '+str(F1))\n",
    "        print('+++++++++++++++++++++++++++++++++++++++++++++')\n",
    "        \n",
    "        initial_features, initial_dataset = construct_features(users, initial_dataset, start_index, len(users), next_time_step, current_features=initial_features)\n",
    "        \n",
    "        model = incremental_trained_model(start_index, users, initial_features, model)\n",
    "#   ============================\n",
    "\n",
    "        current_time = next_time_step\n",
    "    network_simulation.to_csv(simulation_result_path,index=False)\n",
    "    print(f\"Simulation finished after {round((time.time() - start_time)/60,2)} minutes\")\n",
    "\n",
    "    \n",
    "    true_predicted = 0\n",
    "    for each in infected_record:\n",
    "        if each > ini_start_index and each< end_index:\n",
    "            true_predicted+=1\n",
    "    precision = safe_division(true_predicted,len(infected_record)) \n",
    "    recall = safe_division(true_predicted, end_index - ini_start_index) \n",
    "    F1 = safe_division(2.0*precision*recall, precision+recall)\n",
    "    print(str(end_index - ini_start_index)+' users infected in reality')\n",
    "    print('total positive prediction: '+str(len(infected_record)))\n",
    "    print('total true posisive: '+str(true_predicted))\n",
    "    print('total false posisive: '+str(len(infected_record)-true_predicted))\n",
    "    print('precision: '+str( precision ))\n",
    "    print('recall: '+str(recall))\n",
    "    print('F1: '+str(F1))\n",
    "    \n",
    "\n",
    "# Try saving your model after you train on the first batch. Then, on successive runs, provide the xgb.train method with the \n",
    "# filepath of the saved model.\n",
    "\n",
    "# Here's a small experiment that I ran to convince myself that it works:\n",
    "\n",
    "# First, split the boston dataset into training and testing sets. Then split the training set into halves. Fit a model with \n",
    "# the first half and get a score that will serve as a benchmark. Then fit two models with the second half; one model will \n",
    "# have the additional parameter xgb_model. If passing in the extra parameter didn't make a difference, then we would expect \n",
    "# their scores to be similar.. But, fortunately, the new model seems to perform much better than the first.\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from sklearn.cross_validation import train_test_split as ttsplit\n",
    "# from sklearn.datasets import load_boston\n",
    "# from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# X = load_boston()['data']\n",
    "# y = load_boston()['target']\n",
    "\n",
    "# # split data into training and testing sets\n",
    "# # then split training set in half\n",
    "# X_train, X_test, y_train, y_test = ttsplit(X, y, test_size=0.1, random_state=0)\n",
    "# X_train_1, X_train_2, y_train_1, y_train_2 = ttsplit(X_train, \n",
    "#                                                      y_train, \n",
    "#                                                      test_size=0.5,\n",
    "#                                                      random_state=0)\n",
    "\n",
    "# xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\n",
    "# xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\n",
    "# xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# params = {'objective': 'reg:linear', 'verbose': False}\n",
    "# model_1 = xgb.train(params, xg_train_1, 30)\n",
    "# model_1.save_model('model_1.model')\n",
    "\n",
    "# # ================= train two versions of the model =====================#\n",
    "# model_2_v1 = xgb.train(params, xg_train_2, 30)\n",
    "# model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n",
    "\n",
    "# print(mse(model_1.predict(xg_test), y_test))     # benchmark\n",
    "# print(mse(model_2_v1.predict(xg_test), y_test))  # \"before\"\n",
    "# print(mse(model_2_v2.predict(xg_test), y_test))  # \"after\"\n",
    "\n",
    "# 23.0475232194\n",
    "# 39.6776876084\n",
    "# 27.2053239482\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\feature_6_hour.pkl\n",
      "Loaded 5973 entries\n",
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\network_simulation_6_hour.pkl\n",
      "Loaded 5973 entries\n",
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\users.dat\n",
      "Loaded 5973 entries\n",
      "Simulation started\n",
      "current_time:360\n",
      "=================================================\n",
      "there are 3463 infected users.\n",
      "=====progress: 0/3463 users=====\n",
      "user 5334 Infected\n",
      "user 4356 Infected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bird\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:196: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 3937 Infected\n",
      "user 4161 Infected\n",
      "user 4930 Infected\n",
      "user 4688 Infected\n",
      "user 3810 Infected\n",
      "user 4699 Infected\n",
      "user 3587 Infected\n",
      "user 5868 Infected\n",
      "user 4038 Infected\n",
      "user 3643 Infected\n",
      "user 4066 Infected\n",
      "user 3940 Infected\n",
      "user 4944 Infected\n",
      "user 5485 Infected\n",
      "user 3680 Infected\n",
      "user 4474 Infected\n",
      "user 4781 Infected\n",
      "user 3668 Infected\n",
      "user 3722 Infected\n",
      "user 3814 Infected\n",
      "user 4773 Infected\n",
      "user 3674 Infected\n",
      "user 4501 Infected\n",
      "user 3807 Infected\n",
      "user 4547 Infected\n",
      "user 3982 Infected\n",
      "user 4133 Infected\n",
      "user 5126 Infected\n",
      "user 4718 Infected\n",
      "user 4183 Infected\n",
      "user 4874 Infected\n",
      "user 4572 Infected\n",
      "user 5587 Infected\n",
      "user 4834 Infected\n",
      "user 4762 Infected\n",
      "user 3574 Infected\n",
      "user 4668 Infected\n",
      "user 4449 Infected\n",
      "user 3561 Infected\n",
      "user 4249 Infected\n",
      "user 3618 Infected\n",
      "user 3690 Infected\n",
      "user 4096 Infected\n",
      "user 4761 Infected\n",
      "user 4178 Infected\n",
      "user 4320 Infected\n",
      "user 4243 Infected\n",
      "user 3539 Infected\n",
      "user 4437 Infected\n",
      "user 4111 Infected\n",
      "user 3581 Infected\n",
      "user 3837 Infected\n",
      "user 3482 Infected\n",
      "user 3641 Infected\n",
      "user 4618 Infected\n",
      "user 3845 Infected\n",
      "user 3903 Infected\n",
      "user 3599 Infected\n",
      "user 3995 Infected\n",
      "user 3785 Infected\n",
      "user 3658 Infected\n",
      "user 5912 Infected\n",
      "user 3692 Infected\n",
      "user 4156 Infected\n",
      "user 3627 Infected\n",
      "user 4844 Infected\n",
      "user 4955 Infected\n",
      "user 3868 Infected\n",
      "user 3636 Infected\n",
      "user 4188 Infected\n",
      "user 4667 Infected\n",
      "user 4727 Infected\n",
      "user 4504 Infected\n",
      "user 3867 Infected\n",
      "user 3670 Infected\n",
      "user 3487 Infected\n",
      "user 4805 Infected\n",
      "user 3922 Infected\n",
      "user 4638 Infected\n",
      "user 4657 Infected\n",
      "user 4420 Infected\n",
      "user 3854 Infected\n",
      "user 4015 Infected\n",
      "user 3950 Infected\n",
      "user 4584 Infected\n",
      "user 4065 Infected\n",
      "user 4801 Infected\n",
      "user 3773 Infected\n",
      "user 4393 Infected\n",
      "user 4239 Infected\n",
      "user 3682 Infected\n",
      "user 3730 Infected\n",
      "user 4348 Infected\n",
      "user 3600 Infected\n",
      "user 4099 Infected\n",
      "user 3850 Infected\n",
      "user 4295 Infected\n",
      "user 4069 Infected\n",
      "user 4384 Infected\n",
      "user 4302 Infected\n",
      "user 3980 Infected\n",
      "user 4030 Infected\n",
      "user 4047 Infected\n",
      "user 4461 Infected\n",
      "user 4076 Infected\n",
      "user 4131 Infected\n",
      "user 4653 Infected\n",
      "user 4714 Infected\n",
      "user 3760 Infected\n",
      "user 4270 Infected\n",
      "user 4396 Infected\n",
      "user 4913 Infected\n",
      "user 3902 Infected\n",
      "user 3694 Infected\n",
      "user 3672 Infected\n",
      "user 4462 Infected\n",
      "user 3580 Infected\n",
      "user 4018 Infected\n",
      "user 3944 Infected\n",
      "user 3938 Infected\n",
      "user 4143 Infected\n",
      "user 4578 Infected\n",
      "user 4842 Infected\n",
      "user 4463 Infected\n",
      "user 3895 Infected\n",
      "user 5303 Infected\n",
      "user 4723 Infected\n",
      "user 3705 Infected\n",
      "user 3488 Infected\n",
      "user 4487 Infected\n",
      "user 4438 Infected\n",
      "user 4431 Infected\n",
      "user 3466 Infected\n",
      "user 4072 Infected\n",
      "user 4464 Infected\n",
      "user 4136 Infected\n",
      "user 5129 Infected\n",
      "user 4899 Infected\n",
      "user 3536 Infected\n",
      "user 4176 Infected\n",
      "user 3571 Infected\n",
      "user 3788 Infected\n",
      "user 3898 Infected\n",
      "user 3723 Infected\n",
      "user 3989 Infected\n",
      "user 3667 Infected\n",
      "user 4007 Infected\n",
      "user 4583 Infected\n",
      "user 3820 Infected\n",
      "user 4071 Infected\n",
      "user 3833 Infected\n",
      "user 4481 Infected\n",
      "user 3862 Infected\n",
      "user 4137 Infected\n",
      "user 4771 Infected\n",
      "user 4704 Infected\n",
      "user 4924 Infected\n",
      "user 4497 Infected\n",
      "user 4512 Infected\n",
      "user 4820 Infected\n",
      "user 5431 Infected\n",
      "user 4752 Infected\n",
      "user 4219 Infected\n",
      "user 4190 Infected\n",
      "user 4170 Infected\n",
      "user 4286 Infected\n",
      "user 4850 Infected\n",
      "user 3735 Infected\n",
      "user 4492 Infected\n",
      "user 4456 Infected\n",
      "user 4477 Infected\n",
      "user 4142 Infected\n",
      "user 4404 Infected\n",
      "user 4539 Infected\n",
      "user 3794 Infected\n",
      "user 4846 Infected\n",
      "user 4711 Infected\n",
      "user 4496 Infected\n",
      "user 4672 Infected\n",
      "user 4263 Infected\n",
      "user 4555 Infected\n",
      "user 4615 Infected\n",
      "user 4386 Infected\n",
      "user 4890 Infected\n",
      "user 4242 Infected\n",
      "user 4351 Infected\n",
      "user 4377 Infected\n",
      "user 4852 Infected\n",
      "user 4422 Infected\n",
      "user 4893 Infected\n",
      "user 3731 Infected\n",
      "user 4625 Infected\n",
      "user 3671 Infected\n",
      "user 4388 Infected\n",
      "user 3896 Infected\n",
      "user 3747 Infected\n",
      "user 3767 Infected\n",
      "user 4293 Infected\n",
      "user 3958 Infected\n",
      "user 5702 Infected\n",
      "user 3697 Infected\n",
      "user 4775 Infected\n",
      "user 3480 Infected\n",
      "user 4205 Infected\n",
      "user 3919 Infected\n",
      "user 4824 Infected\n",
      "user 3700 Infected\n",
      "user 3861 Infected\n",
      "user 3526 Infected\n",
      "user 3594 Infected\n",
      "user 4181 Infected\n",
      "user 3964 Infected\n",
      "user 3939 Infected\n",
      "user 3790 Infected\n",
      "user 4977 Infected\n",
      "user 3878 Infected\n",
      "user 4050 Infected\n",
      "user 4083 Infected\n",
      "user 4011 Infected\n",
      "user 3984 Infected\n",
      "user 3756 Infected\n",
      "user 3628 Infected\n",
      "user 4735 Infected\n",
      "user 4796 Infected\n",
      "user 4171 Infected\n",
      "user 4445 Infected\n",
      "user 4558 Infected\n",
      "user 3923 Infected\n",
      "user 4569 Infected\n",
      "user 3874 Infected\n",
      "user 4116 Infected\n",
      "user 4573 Infected\n",
      "user 3974 Infected\n",
      "user 4418 Infected\n",
      "user 4532 Infected\n",
      "user 3554 Infected\n",
      "user 3793 Infected\n",
      "user 3915 Infected\n",
      "user 3926 Infected\n",
      "user 4857 Infected\n",
      "user 4990 Infected\n",
      "user 3547 Infected\n",
      "user 4901 Infected\n",
      "user 4608 Infected\n",
      "user 4224 Infected\n",
      "user 3537 Infected\n",
      "user 3954 Infected\n",
      "user 4316 Infected\n",
      "user 4619 Infected\n",
      "user 4216 Infected\n",
      "user 3497 Infected\n",
      "=====progress: 1000/3463 users=====\n",
      "user 4799 Infected\n",
      "user 3880 Infected\n",
      "user 4736 Infected\n",
      "user 4407 Infected\n",
      "user 4722 Infected\n",
      "user 3959 Infected\n",
      "user 4430 Infected\n",
      "user 4788 Infected\n",
      "user 4470 Infected\n",
      "user 4674 Infected\n",
      "user 4389 Infected\n",
      "user 3478 Infected\n",
      "user 4655 Infected\n",
      "user 3622 Infected\n",
      "user 4428 Infected\n",
      "user 3776 Infected\n",
      "user 4888 Infected\n",
      "user 3894 Infected\n",
      "user 4956 Infected\n",
      "user 5594 Infected\n",
      "user 3584 Infected\n",
      "user 4138 Infected\n",
      "user 4634 Infected\n",
      "user 4480 Infected\n",
      "user 3971 Infected\n",
      "user 3579 Infected\n",
      "user 4881 Infected\n",
      "user 3578 Infected\n",
      "user 4571 Infected\n",
      "user 4425 Infected\n",
      "user 4055 Infected\n",
      "user 4087 Infected\n",
      "user 4100 Infected\n",
      "user 4146 Infected\n",
      "user 4931 Infected\n",
      "user 3751 Infected\n",
      "user 3900 Infected\n",
      "user 4410 Infected\n",
      "user 4869 Infected\n",
      "user 3787 Infected\n",
      "user 4327 Infected\n",
      "user 4233 Infected\n",
      "user 4649 Infected\n",
      "user 3873 Infected\n",
      "user 3544 Infected\n",
      "user 5356 Infected\n",
      "user 3666 Infected\n",
      "user 3592 Infected\n",
      "user 4276 Infected\n",
      "user 4110 Infected\n",
      "user 3503 Infected\n",
      "user 3707 Infected\n",
      "user 4712 Infected\n",
      "user 3746 Infected\n",
      "user 3577 Infected\n",
      "user 3605 Infected\n",
      "user 5441 Infected\n",
      "user 3506 Infected\n",
      "user 3465 Infected\n",
      "user 3780 Infected\n",
      "user 3799 Infected\n",
      "user 3905 Infected\n",
      "user 5323 Infected\n",
      "user 4387 Infected\n",
      "user 4947 Infected\n",
      "user 4439 Infected\n",
      "user 4910 Infected\n",
      "user 4792 Infected\n",
      "user 4561 Infected\n",
      "user 5031 Infected\n",
      "user 4515 Infected\n",
      "user 4971 Infected\n",
      "user 4525 Infected\n",
      "user 4432 Infected\n",
      "=====progress: 2000/3463 users=====\n",
      "user 3973 Infected\n",
      "user 3500 Infected\n",
      "user 4755 Infected\n",
      "user 4670 Infected\n",
      "user 4750 Infected\n",
      "user 4938 Infected\n",
      "user 4588 Infected\n",
      "user 4509 Infected\n",
      "user 4229 Infected\n",
      "user 4494 Infected\n",
      "user 3904 Infected\n",
      "user 3781 Infected\n",
      "user 4330 Infected\n",
      "user 4448 Infected\n",
      "user 4936 Infected\n",
      "user 4238 Infected\n",
      "user 3521 Infected\n",
      "user 3841 Infected\n",
      "user 4151 Infected\n",
      "user 3560 Infected\n",
      "user 4941 Infected\n",
      "user 3519 Infected\n",
      "user 3979 Infected\n",
      "user 4790 Infected\n",
      "user 4221 Infected\n",
      "user 4139 Infected\n",
      "user 4984 Infected\n",
      "user 4063 Infected\n",
      "user 3675 Infected\n",
      "user 4315 Infected\n",
      "user 4535 Infected\n",
      "user 4680 Infected\n",
      "user 3724 Infected\n",
      "user 3610 Infected\n",
      "user 4604 Infected\n",
      "user 4115 Infected\n",
      "user 4794 Infected\n",
      "user 4247 Infected\n",
      "user 3710 Infected\n",
      "user 4145 Infected\n",
      "user 4026 Infected\n",
      "user 3654 Infected\n",
      "user 4841 Infected\n",
      "user 4875 Infected\n",
      "user 4036 Infected\n",
      "user 4067 Infected\n",
      "user 4476 Infected\n",
      "user 3657 Infected\n",
      "user 4246 Infected\n",
      "user 4411 Infected\n",
      "user 3975 Infected\n",
      "user 3796 Infected\n",
      "user 3541 Infected\n",
      "user 4853 Infected\n",
      "user 4208 Infected\n",
      "user 4741 Infected\n",
      "user 4914 Infected\n",
      "user 4158 Infected\n",
      "user 4698 Infected\n",
      "user 3678 Infected\n",
      "user 4248 Infected\n",
      "user 4419 Infected\n",
      "user 4851 Infected\n",
      "user 4040 Infected\n",
      "user 3662 Infected\n",
      "user 4663 Infected\n",
      "user 3758 Infected\n",
      "user 3970 Infected\n",
      "=====progress: 3000/3463 users=====\n",
      "user 4104 Infected\n",
      "user 4366 Infected\n",
      "user 3495 Infected\n",
      "user 4217 Infected\n",
      "user 4236 Infected\n",
      "user 4660 Infected\n",
      "user 3717 Infected\n",
      "user 4949 Infected\n",
      "user 4662 Infected\n",
      "user 3829 Infected\n",
      "user 4484 Infected\n",
      "user 3564 Infected\n",
      "user 3557 Infected\n",
      "user 4576 Infected\n",
      "user 3849 Infected\n",
      "user 3942 Infected\n",
      "user 3987 Infected\n",
      "user 4536 Infected\n",
      "user 4807 Infected\n",
      "user 3673 Infected\n",
      "user 3817 Infected\n",
      "user 4289 Infected\n",
      "user 4299 Infected\n",
      "user 4644 Infected\n",
      "user 3782 Infected\n",
      "user 4742 Infected\n",
      "user 4089 Infected\n",
      "user 3527 Infected\n",
      "user 3494 Infected\n",
      "user 4035 Infected\n",
      "user 3876 Infected\n",
      "user 4257 Infected\n",
      "user 3996 Infected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 3869 Infected\n",
      "user 4785 Infected\n",
      "user 4124 Infected\n",
      "user 4009 Infected\n",
      "user 4182 Infected\n",
      "user 4666 Infected\n",
      "user 3965 Infected\n",
      "user 4906 Infected\n",
      "user 3872 Infected\n",
      "user 4195 Infected\n",
      "user 3520 Infected\n",
      "user 4288 Infected\n",
      "user 5000 Infected\n",
      "user 4234 Infected\n",
      "user 3962 Infected\n",
      "user 4362 Infected\n",
      "user 3529 Infected\n",
      "user 4616 Infected\n",
      "user 4806 Infected\n",
      "user 3955 Infected\n",
      "user 4594 Infected\n",
      "user 5070 Infected\n",
      "user 4513 Infected\n",
      "user 4353 Infected\n",
      "user 4902 Infected\n",
      "user 3858 Infected\n",
      "user 5321 Infected\n",
      "user 3887 Infected\n",
      "user 3603 Infected\n",
      "user 4830 Infected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2510 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++At simulation period 360 to 390++++++++\n",
      "146 users infected in reality\n",
      "total positive prediction:  458\n",
      "total true posisive: 45\n",
      "total false posisive: 413\n",
      "precision: 0.0982532751091703\n",
      "recall: 0.3082191780821918\n",
      "F1: 0.14900662251655628\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2510/2510 [00:18<00:00, 137.22it/s]\n",
      "C:\\Users\\bird\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] data counts, with uninfected (0): 2363, infected (1): 147\n",
      "1.0    147\n",
      "0.0    147\n",
      "Name: label, dtype: int64\n",
      "current_time:390\n",
      "=================================================\n",
      "there are 3921 infected users.\n",
      "=====progress: 0/3921 users=====\n",
      "user 4498 Infected\n",
      "user 5830 Infected\n",
      "user 3752 Infected\n",
      "user 4313 Infected\n",
      "user 4524 Infected\n",
      "=====progress: 1000/3921 users=====\n",
      "user 5098 Infected\n",
      "user 3573 Infected\n",
      "=====progress: 2000/3921 users=====\n",
      "user 4090 Infected\n",
      "=====progress: 3000/3921 users=====\n",
      "user 4338 Infected\n",
      "user 3759 Infected\n",
      "user 3843 Infected\n",
      "user 4121 Infected\n",
      "user 4157 Infected\n",
      "user 4800 Infected\n",
      "user 4334 Infected\n",
      "user 3642 Infected\n",
      "user 4725 Infected\n",
      "user 4093 Infected\n",
      "user 4103 Infected\n",
      "user 5225 Infected\n",
      "user 4885 Infected\n",
      "user 5960 Infected\n",
      "user 4595 Infected\n",
      "user 4078 Infected\n",
      "user 5600 Infected\n",
      "user 4860 Infected\n",
      "user 3836 Infected\n",
      "user 3727 Infected\n",
      "user 3806 Infected\n",
      "user 3819 Infected\n",
      "user 3925 Infected\n",
      "user 4199 Infected\n",
      "user 4311 Infected\n",
      "user 3857 Infected\n",
      "user 3912 Infected\n",
      "user 4014 Infected\n",
      "user 4046 Infected\n",
      "user 4321 Infected\n",
      "user 4848 Infected\n",
      "user 4193 Infected\n",
      "user 4287 Infected\n",
      "user 4237 Infected\n",
      "user 4184 Infected\n",
      "user 4685 Infected\n",
      "user 4223 Infected\n",
      "user 4937 Infected\n",
      "user 4041 Infected\n",
      "user 4797 Infected\n",
      "user 4345 Infected\n",
      "user 4230 Infected\n",
      "user 4490 Infected\n",
      "user 4259 Infected\n",
      "user 4426 Infected\n",
      "user 4452 Infected\n",
      "user 4360 Infected\n",
      "user 4346 Infected\n",
      "user 4538 Infected\n",
      "user 4435 Infected\n",
      "user 3512 Infected\n",
      "user 4810 Infected\n",
      "user 4380 Infected\n",
      "user 5764 Infected\n",
      "user 4630 Infected\n",
      "user 4643 Infected\n",
      "user 4654 Infected\n",
      "user 4200 Infected\n",
      "user 4429 Infected\n",
      "user 4424 Infected\n",
      "user 4495 Infected\n",
      "user 4444 Infected\n",
      "user 4450 Infected\n",
      "user 4128 Infected\n",
      "user 4522 Infected\n",
      "user 4341 Infected\n",
      "user 4540 Infected\n",
      "user 4551 Infected\n",
      "user 4864 Infected\n",
      "user 4673 Infected\n",
      "user 4671 Infected\n",
      "user 3928 Infected\n",
      "user 4763 Infected\n",
      "user 4999 Infected\n",
      "user 4489 Infected\n",
      "user 3999 Infected\n",
      "user 3911 Infected\n",
      "user 4091 Infected\n",
      "user 4987 Infected\n",
      "user 4039 Infected\n",
      "user 3613 Infected\n",
      "user 3711 Infected\n",
      "user 4780 Infected\n",
      "user 3823 Infected\n",
      "user 4889 Infected\n",
      "user 4871 Infected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2363 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++At simulation period 390 to 420++++++++\n",
      "100 users infected in reality\n",
      "total positive prediction:  94\n",
      "total true posisive: 2\n",
      "total false posisive: 92\n",
      "precision: 0.02127659574468085\n",
      "recall: 0.02\n",
      "F1: 0.020618556701030927\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2363/2363 [00:17<00:00, 136.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] data counts, with uninfected (0): 2262, infected (1): 101\n",
      "1.0    101\n",
      "0.0    101\n",
      "Name: label, dtype: int64\n",
      "current_time:420\n",
      "=================================================\n",
      "there are 4015 infected users.\n",
      "=====progress: 0/4015 users=====\n",
      "user 4241 Infected\n",
      "user 4317 Infected\n",
      "=====progress: 1000/4015 users=====\n",
      "=====progress: 2000/4015 users=====\n",
      "user 4274 Infected\n",
      "user 5161 Infected\n",
      "=====progress: 3000/4015 users=====\n",
      "user 4127 Infected\n",
      "user 5118 Infected\n",
      "user 4197 Infected\n",
      "user 3534 Infected\n",
      "user 3871 Infected\n",
      "user 4107 Infected\n",
      "user 4260 Infected\n",
      "user 4934 Infected\n",
      "user 4357 Infected\n",
      "user 4365 Infected\n",
      "user 4042 Infected\n",
      "user 3608 Infected\n",
      "user 3719 Infected\n",
      "user 4809 Infected\n",
      "user 4878 Infected\n",
      "user 3908 Infected\n",
      "=====progress: 4000/4015 users=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2262 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++At simulation period 420 to 450++++++++\n",
      "93 users infected in reality\n",
      "total positive prediction:  20\n",
      "total true posisive: 1\n",
      "total false posisive: 19\n",
      "precision: 0.05\n",
      "recall: 0.010752688172043012\n",
      "F1: 0.017699115044247787\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2262/2262 [00:16<00:00, 138.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] data counts, with uninfected (0): 2168, infected (1): 94\n",
      "1.0    94\n",
      "0.0    94\n",
      "Name: label, dtype: int64\n",
      "current_time:450\n",
      "=================================================\n",
      "there are 4035 infected users.\n",
      "=====progress: 0/4035 users=====\n",
      "user 3772 Infected\n",
      "user 4468 Infected\n",
      "user 5141 Infected\n",
      "user 4740 Infected\n",
      "user 3840 Infected\n",
      "=====progress: 1000/4035 users=====\n",
      "user 4342 Infected\n",
      "user 4920 Infected\n",
      "user 5801 Infected\n",
      "=====progress: 2000/4035 users=====\n",
      "=====progress: 3000/4035 users=====\n",
      "user 3568 Infected\n",
      "user 5283 Infected\n",
      "user 5622 Infected\n",
      "user 3848 Infected\n",
      "user 4298 Infected\n",
      "user 4332 Infected\n",
      "user 4322 Infected\n",
      "user 4523 Infected\n",
      "user 4029 Infected\n",
      "=====progress: 4000/4035 users=====\n",
      "++++++++At simulation period 450 to 480++++++++\n",
      "111 users infected in reality\n",
      "total positive prediction:  17\n",
      "total true posisive: 2\n",
      "total false posisive: 15\n",
      "precision: 0.11764705882352941\n",
      "recall: 0.018018018018018018\n",
      "F1: 0.03125\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2168/2168 [00:16<00:00, 128.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] data counts, with uninfected (0): 2056, infected (1): 112\n",
      "1.0    112\n",
      "0.0    112\n",
      "Name: label, dtype: int64\n",
      "current_time:480\n",
      "=================================================\n",
      "there are 4052 infected users.\n",
      "=====progress: 0/4052 users=====\n",
      "user 5855 Infected\n",
      "=====progress: 1000/4052 users=====\n",
      "user 4482 Infected\n",
      "=====progress: 2000/4052 users=====\n",
      "user 5926 Infected\n",
      "=====progress: 3000/4052 users=====\n",
      "user 3766 Infected\n",
      "user 4598 Infected\n",
      "=====progress: 4000/4052 users=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/2056 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++At simulation period 480 to 510++++++++\n",
      "92 users infected in reality\n",
      "total positive prediction:  5\n",
      "total true posisive: 0\n",
      "total false posisive: 5\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "F1: 0\n",
      "+++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2056/2056 [00:16<00:00, 127.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] data counts, with uninfected (0): 1963, infected (1): 93\n",
      "1.0    93\n",
      "0.0    93\n",
      "Name: label, dtype: int64\n",
      "Simulation finished after 12.89 minutes\n",
      "546 users infected in reality\n",
      "total positive prediction: 594\n",
      "total true posisive: 201\n",
      "total false posisive: 393\n",
      "precision: 0.3383838383838384\n",
      "recall: 0.36813186813186816\n",
      "F1: 0.3526315789473684\n"
     ]
    }
   ],
   "source": [
    "network_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl'\n",
    "initial_features_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "user_data_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "model_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\trained_model.dat'\n",
    "simulation_result_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\simulation_result_6_hour.csv'\n",
    "total_time_duration = 510\n",
    "interval = 30\n",
    "current_time = 360\n",
    "\n",
    "model_params = None\n",
    "\n",
    "simulation_process(current_time, total_time_duration, interval, initial_features_path, network_path, user_data_path, model_path, model_params, simulation_result_path)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
