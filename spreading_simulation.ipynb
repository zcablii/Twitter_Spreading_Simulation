{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn import svm\n",
    "from pandas import *\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path = \"F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\\"\n",
    "# event = 'givenchy'\n",
    "# interval = 30\n",
    "# current_time = 360\n",
    "# start_hour = 6\n",
    "\n",
    "\n",
    "def load_pickle_file(pickled_file):\n",
    "    print(f'Loading data file from {pickled_file}')\n",
    "    infile = open(pickled_file,'rb')\n",
    "    unpickled_file = pickle.load(infile)\n",
    "    print(f'Loaded {len(unpickled_file)} entries')\n",
    "    infile.close()\n",
    "    return unpickled_file\n",
    "          \n",
    "    \n",
    "def save_pickle_file(path, data):\n",
    "    print('Dumping data to path {}'.format(path))\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "    print('Finished dumping data to path {}'.format(path))\n",
    "\n",
    "\n",
    "def mean(numbers):\n",
    "    return float(sum(numbers)) / max(len(numbers), 1)\n",
    "\n",
    "\n",
    "def safe_division(x, y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x/y\n",
    "\n",
    "    \n",
    "# users = load_pickle_file(path+\"users.dat\")\n",
    "# users.reset_index(drop =True , inplace =True)\n",
    "# # print(users)\n",
    "# print(users.columns)\n",
    "# print(users['source_candidates'])\n",
    "\n",
    "\n",
    "# print(unique_users)\n",
    "def id_exists(unique_users, uid):\n",
    "    if uid in unique_users:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_id_list(unique_users, value):\n",
    "        return list(filter(lambda uid: id_exists(unique_users,uid), value))\n",
    "    \n",
    "\n",
    "def network_simulation_init(users, start_hour):\n",
    "    current_time = start_hour*60 \n",
    "    network_simulation = pd.DataFrame(columns= ['id','time_lapsed','favourites_count','followers_count','friends_count',\n",
    "                                'listed_count','statuses_count','source_candidates','source_index','seed_index','generation',\n",
    "                                 'time_since_seed','user_created_days','normalized_statuses_count','normalized_followers_count',\n",
    "                                 'normalized_favourites_count','normalized_listed_count','normalized_friends_count'])\n",
    "\n",
    "    network_simulation['id']=users['id']\n",
    "    network_simulation['favourites_count']=users['favourites_count']\n",
    "    network_simulation['followers_count']=users['followers_count']\n",
    "    network_simulation['friends_count']=users['friends_count']\n",
    "    network_simulation['listed_count']=users['listed_count']\n",
    "    network_simulation['statuses_count']=users['statuses_count']\n",
    "    network_simulation['user_created_days']=users['user_created_days']\n",
    "    network_simulation['normalized_statuses_count']=users['normalized_statuses_count']\n",
    "    network_simulation['normalized_followers_count']=users['normalized_followers_count']\n",
    "    network_simulation['normalized_favourites_count']=users['normalized_favourites_count']\n",
    "    network_simulation['normalized_listed_count']=users['normalized_listed_count']\n",
    "    network_simulation['normalized_friends_count']=users['normalized_friends_count']\n",
    "#     network_simulation['followers_list'] =users['followers_json'] \n",
    "#     network_simulation['friends_list'] =users['friends_json']\n",
    "    network_simulation['time_lapsed'] = users['time_lapsed'].apply(lambda x: x if x <= current_time else None)\n",
    "    network_simulation['source_index'] = users.apply(lambda x: x['source_index'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['seed_index'] = users.apply(lambda x: x['seed_index'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['generation'] = users.apply(lambda x: x['generation'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "    network_simulation['time_since_seed'] = users.apply(lambda x: x['time_since_seed'] if x['time_lapsed'] <= current_time else None,axis=1)\n",
    "\n",
    "# print(\"network_simulation\")\n",
    "# print(network_simulation)\n",
    "    return network_simulation\n",
    "\n",
    "\n",
    "def network_simulation_merge_friends_list(network_simulation, path_of_friends_list):\n",
    "\n",
    "    friends = load_pickle_file(path_of_friends_list)\n",
    "    friends_list = pd.DataFrame(columns=['id'], data=friends.keys())\n",
    "    friends_list[\"friends_list\"] = ''\n",
    "    for i in range (0, len(friends)):\n",
    "        friends_list[\"friends_list\"][i]= friends.get(friends_list[\"id\"][i])\n",
    "    friends_list.reset_index(drop =True , inplace =True)\n",
    "    return pd.merge(network_simulation,friends_list, on='id', how='left')\n",
    "    \n",
    "\n",
    "def network_simulation_merge_followers_list(network_simulation, path_of_followers_list):\n",
    "    followers = load_pickle_file(path_of_followers_list)\n",
    "    followers_list = pd.DataFrame(columns=['id'], data=followers.keys())\n",
    "    followers_list[\"followers_list\"] = ''\n",
    "    for i in range (0, len(followers)):\n",
    "        followers_list[\"followers_list\"][i]= followers.get(followers_list[\"id\"][i])\n",
    "    # print(friends_list.get(942362499923566592 ))\n",
    "    followers_list.reset_index(drop =True , inplace =True)\n",
    "    # print(followers_list)\n",
    "\n",
    "\n",
    "    return pd.merge(network_simulation,followers_list,on='id',how='left')\n",
    "    \n",
    "#     print(network_simulation)\n",
    "\n",
    "\n",
    "\n",
    "def process_data(users, network_simulation, start_index, end_index,start_hour):\n",
    "    \n",
    "    current_time = start_hour*60   \n",
    "    in_degree = list(users.friends_count)\n",
    "    out_degree = list(users.followers_count)\n",
    "    degree = (users.friends_count+users.followers_count).tolist()\n",
    "    \n",
    "    features = {\n",
    "        #Columns which are added for simulation, but they are not used as features for model prediction\n",
    "        'user_id':[],\n",
    "        'infected_status':[],\n",
    "        'infection_time':[],\n",
    "        'followers_list':[],\n",
    "        \n",
    "        #Columns used as features for model prediction   33\n",
    "        'UsM_deltaDays': [],\n",
    "        'UsM_statusesCount': [],\n",
    "        'UsM_followersCount': [],\n",
    "        'UsM_favouritesCount': [],\n",
    "        'UsM_friendsCount': [],\n",
    "        'UsM_listedCount': [],\n",
    "        'UsM_normalizedUserStatusesCount': [],\n",
    "        'UsM_normalizedUserFollowersCount': [],\n",
    "        'UsM_normalizedUserFavouritesCount': [],\n",
    "        'UsM_normalizedUserListedCount': [],\n",
    "        'UsM_normalizedUserFriendsCount': [],          \n",
    "        'UsM_deltaDays0': [],\n",
    "        'UsM_statusesCount0': [],\n",
    "        'UsM_followersCount0': [],\n",
    "        'UsM_favouritesCount0': [],\n",
    "        'UsM_friendsCount0': [],\n",
    "        'UsM_listedCount0': [],\n",
    "        'UsM_normalizedUserStatusesCount0': [],\n",
    "        'UsM_normalizedUserFollowersCount0': [],\n",
    "        'UsM_normalizedUserFavouritesCount0': [],\n",
    "        'UsM_normalizedUserListedCount0': [],\n",
    "        'UsM_normalizedUserFriendsCount0': [],\n",
    "        'UsM_deltaDays-1': [],\n",
    "        'UsM_statusesCount-1': [],\n",
    "        'UsM_followersCount-1': [],\n",
    "        'UsM_favouritesCount-1': [],\n",
    "        'UsM_friendsCount-1': [],\n",
    "        'UsM_listedCount-1': [],\n",
    "        'UsM_normalizedUserStatusesCount-1': [],\n",
    "        'UsM_normalizedUserFollowersCount-1': [],\n",
    "        'UsM_normalizedUserFavouritesCount-1': [],\n",
    "        'UsM_normalizedUserListedCount-1': [],\n",
    "        'UsM_normalizedUserFriendsCount-1': [],\n",
    "        # TwM: Tweet metadata   5\n",
    "        'TwM_t0': [],\n",
    "        'TwM_tSeed0': [],\n",
    "        'TwM_t-1': [],\n",
    "        'TwM_tSeed-1': [],\n",
    "        'TwM_tCurrent': [],\n",
    "        # Nw: Network0    15\n",
    "        'Nw_degree': [],\n",
    "        'Nw_inDegree': [],\n",
    "        'Nw_outDegree': [],\n",
    "        'Nw_degree0': [],\n",
    "        'Nw_inDegree0': [],\n",
    "        'Nw_outDegree0': [],\n",
    "        'Nw_degree-1': [],\n",
    "        'Nw_inDegree-1': [],\n",
    "        'Nw_outDegree-1': [],\n",
    "        'Nw_degreeSeed0': [],\n",
    "        'Nw_inDegreeSeed0': [],\n",
    "        'Nw_outDegreeSeed0': [],\n",
    "        'Nw_degreeSeed-1': [],\n",
    "        'Nw_inDegreeSeed-1': [],\n",
    "        'Nw_outDegreeSeed-1': [],\n",
    "        # SNw: Spreading Network   20\n",
    "        'SNw_nFriendsInfected': [],\n",
    "        'SNw_friendsInfectedRatio': [],\n",
    "        'SNw_generation0': [],\n",
    "        'SNw_generation-1': [],\n",
    "        'SNw_timeSinceSeed0': [],\n",
    "        'SNw_timeSinceSeed-1': [],\n",
    "        'SNw_totalNodesInfected': [],\n",
    "        'SNw_nodeInfectedCentrality': [],\n",
    "        'SNw_totalInDegree': [],\n",
    "        'SNw_totalOutDegree': [],\n",
    "        'SNw_inDegreeCentrality': [],\n",
    "        'SNw_inDegreeCentrality0': [],\n",
    "        'SNw_inDegreeCentrality-1': [],\n",
    "        'SNw_outDegreeCentrality': [],\n",
    "        'SNw_outDegreeCentrality0': [],\n",
    "        'SNw_outDegreeCentrality-1': [],\n",
    "        'SNw_inDegreeCentralitySeed0':[],\n",
    "        'SNw_outDegreeCentralitySeed0':[],\n",
    "        'SNw_inDegreeCentralitySeed-1':[],\n",
    "        'SNw_outDegreeCentralitySeed-1':[],\n",
    "        # Stat: Statistical  15\n",
    "        'Stat_average_kOut': [],\n",
    "        'Stat_average_t': [],\n",
    "        'Stat_average_deltaDays': [],\n",
    "        'Stat_average_statusesCount': [],\n",
    "        'Stat_average_followersCount': [],\n",
    "        'Stat_average_favouritesCount': [],\n",
    "        'Stat_average_friendsCount': [],\n",
    "        'Stat_average_listedCount': [],\n",
    "        'Stat_average_normalizedUserStatusesCount': [],\n",
    "        'Stat_average_normalizedUserFollowersCount': [],\n",
    "        'Stat_average_normalizedUserFavouritesCount': [],\n",
    "        'Stat_average_normalizedUserListedCount': [],\n",
    "        'Stat_average_normalizedUserFriendsCount': [],                \n",
    "        'Stat_max_kOut': [],\n",
    "        'Stat_min_kOut': []\n",
    "        \n",
    "    }\n",
    "\n",
    "    with tqdm(total=len(list(users[start_index: end_index].iterrows()))) as pbar: \n",
    "        for index, user_row in users[start_index: end_index].iterrows():\n",
    "            source_candidates = sorted(user_row['source_candidates'])\n",
    "            features['user_id'].append(user_row['id'])\n",
    "            features['infected_status'].append(False)\n",
    "            features['infection_time'].append(None)\n",
    "            #print(f\"user_row['followers_list']:{user_row['followers_list']}\")\n",
    "            features['followers_list'].append(user_row['followers_json'])\n",
    "            #print(\"b\")\n",
    "            features['UsM_deltaDays'].append(user_row['user_created_days'])\n",
    "            features['UsM_statusesCount'].append(user_row['statuses_count'])\n",
    "            features['UsM_followersCount'].append(user_row['followers_count'])\n",
    "            features['UsM_favouritesCount'].append(user_row['favourites_count'])\n",
    "            features['UsM_friendsCount'].append(user_row['friends_count'])\n",
    "            features['UsM_listedCount'].append(user_row['listed_count'])\n",
    "            features['UsM_normalizedUserStatusesCount'].append(user_row['normalized_statuses_count'])\n",
    "            features['UsM_normalizedUserFollowersCount'].append(user_row['normalized_followers_count'])\n",
    "            features['UsM_normalizedUserFavouritesCount'].append(user_row['normalized_favourites_count'])\n",
    "            features['UsM_normalizedUserListedCount'].append(user_row['normalized_listed_count'])\n",
    "            features['UsM_normalizedUserFriendsCount'].append(user_row['normalized_friends_count'])\n",
    "            if isinstance(source_candidates, list):\n",
    "                sources = network_simulation.loc[source_candidates]\n",
    "                sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "#                 if user_row['time_lapsed'] > current_time:\n",
    "#                     sources_dataframe = sources[sources['time_lapsed'] <= current_time]\n",
    "#                 else:\n",
    "#                      sources_dataframe = sources[sources['time_lapsed'] <= user_row['time_lapsed']]\n",
    "                sources = sources_dataframe.index.tolist()\n",
    "            else:\n",
    "                sources = []\n",
    "            #sources = [x for x in source_candidates if users.loc[x,'time_lapsed'] <= current_time]\n",
    "            #print(f'sources:{sources}')\n",
    "            if len(sources) > 0:\n",
    "\n",
    "                # Assign the values here to save computation\n",
    "                first_source_index = source_candidates[0]\n",
    "                first_source_row = users.loc[first_source_index]\n",
    "                first_source_seed_row = users.loc[first_source_row['seed_index']]\n",
    "\n",
    "                inDegreeList = sources_dataframe.friends_count.tolist()\n",
    "                outDegreeList = sources_dataframe.followers_count.tolist()\n",
    "                degreeList = [x + y for x, y in zip(inDegreeList, outDegreeList)]\n",
    "                \n",
    "                s_ind = sources_dataframe.friends_count\n",
    "                s_outd = sources_dataframe.followers_count\n",
    "                outDegreeList = s_outd.tolist()\n",
    "                inDegreeList = s_ind.tolist()\n",
    "                degreeList = (s_ind + s_outd).tolist()\n",
    "\n",
    "                #degreeList = list(users.loc[i, 'followers_count'] + users.loc[i, 'friends_count']  for i in sources)\n",
    "                current_time_series = pd.Series([current_time] * len(sources))\n",
    "                time_lapsed_series = sources_dataframe.time_lapsed\n",
    "                timeList = (current_time_series - time_lapsed_series).tolist()\n",
    "\n",
    "\n",
    "                last_source_index = sources[-1]\n",
    "                last_source_row = network_simulation.loc[last_source_index]\n",
    "                last_source_seed_row = network_simulation.loc[last_source_row['seed_index']]\n",
    "\n",
    "                usr_index = index\n",
    "\n",
    "                if user_row['time_lapsed'] <= current_time:\n",
    "\n",
    "                    features['infected_status'][-1] = True\n",
    "                    features['infection_time'][-1] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[usr_index,'time_lapsed'] = user_row['time_lapsed']\n",
    "\n",
    "                    network_simulation.loc[usr_index,'source_index'] = user_row['source_index']\n",
    "                    network_simulation.loc[usr_index,'seed_index'] = user_row['seed_index']\n",
    "                    network_simulation.loc[usr_index, 'generation'] = user_row['generation']\n",
    "                    network_simulation.loc[usr_index, 'time_since_seed'] = user_row['time_since_seed']\n",
    "\n",
    "\n",
    "                network_simulation.at[usr_index,'source_candidates'] = sources\n",
    "\n",
    "                # UsM: User metadata\n",
    "\n",
    "                features['UsM_deltaDays0'].append(first_source_row.user_created_days)\n",
    "                features['UsM_statusesCount0'].append(first_source_row.statuses_count)\n",
    "                features['UsM_followersCount0'].append(first_source_row.followers_count)\n",
    "                features['UsM_favouritesCount0'].append(first_source_row.favourites_count)\n",
    "                features['UsM_friendsCount0'].append(first_source_row.friends_count)\n",
    "                features['UsM_listedCount0'].append(first_source_row.listed_count)\n",
    "                features['UsM_normalizedUserStatusesCount0'].append(first_source_row.normalized_statuses_count)\n",
    "                features['UsM_normalizedUserFollowersCount0'].append(first_source_row.normalized_followers_count)\n",
    "                features['UsM_normalizedUserFavouritesCount0'].append(first_source_row.normalized_favourites_count)\n",
    "                features['UsM_normalizedUserListedCount0'].append(first_source_row.normalized_listed_count)\n",
    "                features['UsM_normalizedUserFriendsCount0'].append(first_source_row.normalized_friends_count)\n",
    "                features['UsM_deltaDays-1'].append(last_source_row.user_created_days)\n",
    "                features['UsM_statusesCount-1'].append(last_source_row.statuses_count)\n",
    "                features['UsM_followersCount-1'].append(last_source_row.followers_count)\n",
    "                features['UsM_favouritesCount-1'].append(last_source_row.favourites_count)\n",
    "                features['UsM_friendsCount-1'].append(last_source_row.friends_count)\n",
    "                features['UsM_listedCount-1'].append(last_source_row.listed_count)\n",
    "                features['UsM_normalizedUserStatusesCount-1'].append(last_source_row.normalized_statuses_count)\n",
    "                features['UsM_normalizedUserFollowersCount-1'].append(last_source_row.normalized_followers_count)\n",
    "                features['UsM_normalizedUserFavouritesCount-1'].append(last_source_row.normalized_favourites_count)\n",
    "                features['UsM_normalizedUserListedCount-1'].append(last_source_row.normalized_listed_count)\n",
    "                features['UsM_normalizedUserFriendsCount-1'].append(last_source_row.normalized_friends_count)\n",
    "                # TwM: Tweet metadata\n",
    "                features['TwM_t0'].append(round(timeList[0], 1))\n",
    "                features['TwM_tSeed0'].append(round(current_time - first_source_seed_row['time_lapsed'], 1))\n",
    "                features['TwM_t-1'].append(round(timeList[-1], 1))\n",
    "                features['TwM_tSeed-1'].append(round(current_time - last_source_seed_row['time_lapsed'], 1))\n",
    "                features['TwM_tCurrent'].append(current_time)\n",
    "                # Nw: Network\n",
    "                features['Nw_degree'].append(degree[index])\n",
    "                features['Nw_inDegree'].append(in_degree[index])\n",
    "                features['Nw_outDegree'].append(out_degree[index])\n",
    "                features['Nw_degree0'].append(degree[first_source_index])\n",
    "                features['Nw_inDegree0'].append(in_degree[first_source_index])\n",
    "                features['Nw_outDegree0'].append(out_degree[first_source_index])\n",
    "                features['Nw_degree-1'].append(degree[last_source_index])\n",
    "                features['Nw_inDegree-1'].append(in_degree[last_source_index])\n",
    "                features['Nw_outDegree-1'].append(out_degree[last_source_index])\n",
    "                features['Nw_degreeSeed0'].append(degree[int(first_source_row['seed_index'])])\n",
    "                features['Nw_inDegreeSeed0'].append(in_degree[int(first_source_row['seed_index'])])\n",
    "                features['Nw_outDegreeSeed0'].append(out_degree[int(first_source_row['seed_index'])])\n",
    "                features['Nw_degreeSeed-1'].append(degree[int(last_source_row['seed_index'])])\n",
    "                features['Nw_inDegreeSeed-1'].append(in_degree[int(last_source_row['seed_index'])])\n",
    "                features['Nw_outDegreeSeed-1'].append(out_degree[int(last_source_row['seed_index'])])\n",
    "                # SNw: Spreading Network\n",
    "                features['SNw_nFriendsInfected'].append(len(sources))\n",
    "                features['SNw_friendsInfectedRatio'].append(safe_division(len(sources), user_row['friends_count']))\n",
    "                features['SNw_generation0'].append(first_source_row['generation'])\n",
    "                features['SNw_generation-1'].append(last_source_row['generation'])\n",
    "                features['SNw_timeSinceSeed0'].append(first_source_row['time_since_seed'])\n",
    "                features['SNw_timeSinceSeed-1'].append(last_source_row['time_since_seed'])\n",
    "\n",
    "                infected_dataframe = network_simulation[network_simulation.time_lapsed <= current_time]\n",
    "                total_nodes_infected = infected_dataframe.shape[0]\n",
    "                total_in_degree = sum(infected_dataframe.friends_count)\n",
    "                total_out_degree = sum(infected_dataframe.followers_count)\n",
    "\n",
    "                features['SNw_totalNodesInfected'].append(total_nodes_infected)\n",
    "                features['SNw_nodeInfectedCentrality'].append(len(sources)/total_nodes_infected)\n",
    "                features['SNw_totalInDegree'].append(total_in_degree)\n",
    "                features['SNw_totalOutDegree'].append(total_out_degree)\n",
    "                features['SNw_inDegreeCentrality'].append(in_degree[index]/total_in_degree)\n",
    "                features['SNw_inDegreeCentrality0'].append(in_degree[first_source_index]/total_in_degree)\n",
    "                features['SNw_inDegreeCentrality-1'].append(in_degree[last_source_index]/total_in_degree)\n",
    "                features['SNw_outDegreeCentrality'].append(out_degree[index]/total_out_degree)\n",
    "                features['SNw_outDegreeCentrality0'].append(out_degree[first_source_index]/total_out_degree)\n",
    "                features['SNw_outDegreeCentrality-1'].append(out_degree[last_source_index]/total_out_degree)\n",
    "                features['SNw_inDegreeCentralitySeed0'].append(in_degree[int(first_source_row['seed_index'])]/total_in_degree)\n",
    "                features['SNw_outDegreeCentralitySeed0'].append(out_degree[int(first_source_row['seed_index'])]/total_out_degree)\n",
    "                features['SNw_inDegreeCentralitySeed-1'].append(in_degree[int(last_source_row['seed_index'])]/total_in_degree)\n",
    "                features['SNw_outDegreeCentralitySeed-1'].append(out_degree[int(last_source_row['seed_index'])]/total_out_degree)\n",
    "                # Stat: Statistical\n",
    "                features['Stat_average_kOut'].append(round(mean(degreeList), 1))\n",
    "                features['Stat_average_t'].append(round(mean(timeList), 1))\n",
    "                features['Stat_average_deltaDays'].append(sources_dataframe.user_created_days.mean())\n",
    "                features['Stat_average_statusesCount'].append(sources_dataframe.statuses_count.mean())\n",
    "                features['Stat_average_followersCount'].append(sources_dataframe.followers_count.mean())\n",
    "                features['Stat_average_favouritesCount'].append(sources_dataframe.favourites_count.mean())\n",
    "                features['Stat_average_friendsCount'].append(sources_dataframe.friends_count.mean())\n",
    "                features['Stat_average_listedCount'].append(sources_dataframe.listed_count.mean())\n",
    "                features['Stat_average_normalizedUserStatusesCount'].append(sources_dataframe.normalized_statuses_count.mean())\n",
    "                features['Stat_average_normalizedUserFollowersCount'].append(sources_dataframe.normalized_followers_count.mean())\n",
    "                features['Stat_average_normalizedUserFavouritesCount'].append(sources_dataframe.normalized_favourites_count.mean())\n",
    "                features['Stat_average_normalizedUserListedCount'].append(sources_dataframe.normalized_listed_count.mean())\n",
    "                features['Stat_average_normalizedUserFriendsCount'].append(sources_dataframe.normalized_friends_count.mean())\n",
    "                features['Stat_max_kOut'].append(max(degreeList))\n",
    "                features['Stat_min_kOut'].append(min(degreeList))\n",
    "            else:\n",
    "                features['UsM_deltaDays0'].append(None)\n",
    "                features['UsM_statusesCount0'].append(None)\n",
    "                features['UsM_followersCount0'].append(None)\n",
    "                features['UsM_favouritesCount0'].append(None)\n",
    "                features['UsM_friendsCount0'].append(None)\n",
    "                features['UsM_listedCount0'].append(None)\n",
    "                features['UsM_normalizedUserStatusesCount0'].append(None)\n",
    "                features['UsM_normalizedUserFollowersCount0'].append(None)\n",
    "                features['UsM_normalizedUserFavouritesCount0'].append(None)\n",
    "                features['UsM_normalizedUserListedCount0'].append(None)\n",
    "                features['UsM_normalizedUserFriendsCount0'].append(None)\n",
    "                features['UsM_deltaDays-1'].append(None)\n",
    "                features['UsM_statusesCount-1'].append(None)\n",
    "                features['UsM_followersCount-1'].append(None)\n",
    "                features['UsM_favouritesCount-1'].append(None)\n",
    "                features['UsM_friendsCount-1'].append(None)\n",
    "                features['UsM_listedCount-1'].append(None)\n",
    "                features['UsM_normalizedUserStatusesCount-1'].append(None)\n",
    "                features['UsM_normalizedUserFollowersCount-1'].append(None)\n",
    "                features['UsM_normalizedUserFavouritesCount-1'].append(None)\n",
    "                features['UsM_normalizedUserListedCount-1'].append(None)\n",
    "                features['UsM_normalizedUserFriendsCount-1'].append(None)\n",
    "                # TwM: Tweet metadata\n",
    "                features['TwM_t0'].append(None)\n",
    "                features['TwM_tSeed0'].append(None)\n",
    "                features['TwM_t-1'].append(None)\n",
    "                features['TwM_tSeed-1'].append(None)\n",
    "                features['TwM_tCurrent'].append(None)\n",
    "                # Nw: Network\n",
    "                features['Nw_degree'].append(None)\n",
    "                features['Nw_inDegree'].append(None)\n",
    "                features['Nw_outDegree'].append(None)\n",
    "                features['Nw_degree0'].append(None)\n",
    "                features['Nw_inDegree0'].append(None)\n",
    "                features['Nw_outDegree0'].append(None)\n",
    "                features['Nw_degree-1'].append(None)\n",
    "                features['Nw_inDegree-1'].append(None)\n",
    "                features['Nw_outDegree-1'].append(None)\n",
    "                features['Nw_degreeSeed0'].append(None)\n",
    "                features['Nw_inDegreeSeed0'].append(None)\n",
    "                features['Nw_outDegreeSeed0'].append(None)\n",
    "                features['Nw_degreeSeed-1'].append(None)\n",
    "                features['Nw_inDegreeSeed-1'].append(None)\n",
    "                features['Nw_outDegreeSeed-1'].append(None)\n",
    "                # SNw: Spreading Network\n",
    "                features['SNw_nFriendsInfected'].append(0)\n",
    "                features['SNw_friendsInfectedRatio'].append(None)\n",
    "                features['SNw_generation0'].append(None)\n",
    "                features['SNw_generation-1'].append(None)\n",
    "                features['SNw_timeSinceSeed0'].append(None)\n",
    "                features['SNw_timeSinceSeed-1'].append(None)\n",
    "                features['SNw_totalNodesInfected'].append(None)\n",
    "                features['SNw_nodeInfectedCentrality'].append(None)\n",
    "                features['SNw_totalInDegree'].append(None)\n",
    "                features['SNw_totalOutDegree'].append(None)\n",
    "                features['SNw_inDegreeCentrality'].append(None)\n",
    "                features['SNw_inDegreeCentrality0'].append(None)\n",
    "                features['SNw_inDegreeCentrality-1'].append(None)\n",
    "                features['SNw_outDegreeCentrality'].append(None)\n",
    "                features['SNw_outDegreeCentrality0'].append(None)\n",
    "                features['SNw_outDegreeCentrality-1'].append(None)\n",
    "                features['SNw_inDegreeCentralitySeed0'].append(None)\n",
    "                features['SNw_outDegreeCentralitySeed0'].append(None)\n",
    "                features['SNw_inDegreeCentralitySeed-1'].append(None)\n",
    "                features['SNw_outDegreeCentralitySeed-1'].append(None)\n",
    "                # Stat: Statistical\n",
    "                features['Stat_average_kOut'].append(None)\n",
    "                features['Stat_average_t'].append(None)\n",
    "                features['Stat_average_deltaDays'].append(None)\n",
    "                features['Stat_average_statusesCount'].append(None)\n",
    "                features['Stat_average_followersCount'].append(None)\n",
    "                features['Stat_average_favouritesCount'].append(None)\n",
    "                features['Stat_average_friendsCount'].append(None)\n",
    "                features['Stat_average_listedCount'].append(None)\n",
    "                features['Stat_average_normalizedUserStatusesCount'].append(None)\n",
    "                features['Stat_average_normalizedUserFollowersCount'].append(None)\n",
    "                features['Stat_average_normalizedUserFavouritesCount'].append(None)\n",
    "                features['Stat_average_normalizedUserListedCount'].append(None)\n",
    "                features['Stat_average_normalizedUserFriendsCount'].append(None)\n",
    "                features['Stat_max_kOut'].append(None)\n",
    "                features['Stat_min_kOut'].append(None)\n",
    "\n",
    "            pbar.update(1)\n",
    "    processed_dataframe = pd.DataFrame(features)\n",
    "    return processed_dataframe\n",
    "\n",
    "def data_preparation_process(path_of_user_data, path_of_follower_list, path_of_friend_list, feature_file_path, network_file_path, start_hour=7):\n",
    "    \n",
    "    users = load_pickle_file(path_of_user_data)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "    unique_users = set(users.index)\n",
    "#     print(users['source_candidates'])\n",
    "    users[\"source_candidates\"] = users[\"source_candidates\"].map(lambda x: filter_id_list(unique_users, x))\n",
    "#     print(users['source_candidates'])\n",
    "    network_simulation = network_simulation_init(users, start_hour)\n",
    "    start_index = 0\n",
    "    end_index = 5973\n",
    "    features = process_data(users, network_simulation, start_index, end_index,start_hour)\n",
    "    network_simulation=network_simulation_merge_friends_list(network_simulation, path_of_friend_list)\n",
    "    network_simulation=network_simulation_merge_followers_list(network_simulation, path_of_follower_list)\n",
    "    print(network_simulation)\n",
    "#     print(network_simulation['source_candidates'])\n",
    "#     print(network_simulation.at[1000,'source_candidates'])\n",
    "    save_pickle_file(feature_file_path,features)\n",
    "    save_pickle_file(network_file_path,network_simulation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"red\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def prepare_training_data(initial_features):\n",
    "    df = initial_features\n",
    "    \n",
    "    df['label']=df['infected_status'].apply(lambda x: 1 if x == True else 0)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.drop(columns = ['user_id', 'infected_status', 'infection_time', 'followers_list'],axis = 1)\n",
    "    \n",
    "    # Converting all type to float, to prepare for feature selection\n",
    "    df = df.astype('float')\n",
    "    # Reset index, with drop equals to true to avoid setting old index as a new column\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Visualize distribution\n",
    "    print('[Original] data counts, with uninfected (0): {}, infected (1): {}'.format(\n",
    "        df['label'].value_counts()[0],\n",
    "        df['label'].value_counts()[1]\n",
    "    ))\n",
    "    df.groupby(['TwM_tCurrent','label']).size().unstack(fill_value=0).plot.bar(title='Original Data Distribution')\n",
    "    \n",
    "    columns = list(df.columns)\n",
    "    columns.remove('label')\n",
    "    \n",
    "    X = df[columns]\n",
    "    y = df[['label']]\n",
    "    return df, X, y\n",
    "\n",
    "def upsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.label==0] # Uninfected is the major class\n",
    "    df_minority = df[df.label==1] # Infected is the minor class\n",
    "\n",
    "    # Upsample minority class\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=len(df_majority),    # to match majority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "    # Display new class counts\n",
    "    print(df_upsampled.label.value_counts())\n",
    "    \n",
    "    return df_upsampled\n",
    "\n",
    "def downsample(df):\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.label==0] # Uninfected is the major class\n",
    "    df_minority = df[df.label==1] # Infected is the minor class\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=len(df_minority),     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    # Display new class counts\n",
    "    print(df_downsampled.label.value_counts())\n",
    "    \n",
    "    return df_downsampled\n",
    "\n",
    "def train(df, X, y, model, params, n_folds, num_boost_round, rebalance_method):\n",
    "    # 2. N Fold Split\n",
    "    # Stratified K-Folds cross-validator\n",
    "    # Provides train/test indices to split data in train/test sets. \n",
    "    # This cross-validation object is a variation of KFold that returns stratified folds. \n",
    "    # The folds are made by preserving the percentage of samples for each class.\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    \n",
    "    corrDataframe = pd.DataFrame()    \n",
    "    mse = []\n",
    "    acc = []\n",
    "    roc = []\n",
    "    F1 = []\n",
    "    auc = []\n",
    "    auc_t = []\n",
    "    acc_t = []\n",
    "    fold_count = 0\n",
    "    t_current = 210\n",
    "    number_of_features = len(X.columns)\n",
    "\n",
    "    print(\"Start cross validation\")\n",
    "    for train, test in skf.split(X, y):\n",
    "        print(\"===Processing fold %s===\" % fold_count)\n",
    "        train_fold = df.loc[train]\n",
    "        test_fold = df.loc[test]\n",
    "\n",
    "        # 3. Rebalance\n",
    "        if rebalance_method == 'up':\n",
    "            train_fold = upsample(train_fold)\n",
    "        if rebalance_method == 'down':\n",
    "            train_fold = downsample(train_fold)\n",
    "         \n",
    "        # 4. Feature Selection\n",
    "#         corr = train_fold.corr()['label'][train_fold.corr()['label'] < 1].abs()\n",
    "#         corr = corr.sort_values(ascending=False)\n",
    "#         corrDataframe = corrDataframe.append(pd.DataFrame(corr.rename('cv{}'.format(fold_count))).T)\n",
    "#         features = corr.index[range(number_of_features)].values\n",
    "        features = X.columns\n",
    "        \n",
    "        # 5. Training\n",
    "        # Fit Model\n",
    "        xgtrain = model.DMatrix(train_fold[features].values, train_fold['label'].values)\n",
    "        xgtest = model.DMatrix(test_fold[features].values, test_fold['label'].values)\n",
    "        evallist = [(xgtrain, 'train'),(xgtest,'eval')]\n",
    "#         evallist = []\n",
    "        \n",
    "        bst = model.train(params, xgtrain, \n",
    "                        num_boost_round = num_boost_round, \n",
    "                        evals = evallist)\n",
    "\n",
    "        # 6. Testing\n",
    "        # Check MSE on test set\n",
    "        \n",
    "        test_fold_t = test_fold[test_fold.TwM_tCurrent == t_current]\n",
    "#             xgtest = xgb.DMatrix(test_fold[features].values)\n",
    "        xgtest_t = model.DMatrix(test_fold_t[features].values)\n",
    "        pred = bst.predict(xgtest)\n",
    "        pred_t = bst.predict(xgtest_t)\n",
    "\n",
    "        mse.append(mean_squared_error(test_fold['label'], pred))\n",
    "        roc.append(roc_auc_score(test_fold['label'], pred))\n",
    "        #auc_t.append(roc_auc_score(test_fold_t['label'], pred_t))\n",
    "\n",
    "        acc.append(accuracy_score(test_fold['label'], (pred>0.5).astype(int)))\n",
    "        acc_t.append(accuracy_score(test_fold_t['label'], (pred_t>0.5).astype(int)))\n",
    "        F1.append(f1_score(test_fold['label'],(pred>0.5).astype(int)))\n",
    "        cm = confusion_matrix(test_fold['label'], (pred>0.5).astype(int))\n",
    "        plot_confusion_matrix(cm, \n",
    "                              normalize    = True,\n",
    "                              target_names = ['Uninfected', 'Infected'],\n",
    "                              title        = \"Confusion Matrix, Normalized\")\n",
    "\n",
    "\n",
    "        fold_count += 1\n",
    "        # Done with the fold\n",
    "    print(\"Finished cross validation\")\n",
    "    print(\"MSE: {} \".format(DataFrame(mse).mean()))\n",
    "    print(\"ACC: {} \".format(DataFrame(acc).mean()))\n",
    "    print(\"AUC: {} \".format(DataFrame(roc).mean()))\n",
    "#     print(\"F1: {} \".format(DataFrame(F1).mean()))\n",
    "    print(\"ACC for t at {}: {} \".format(t_current, DataFrame(acc_t).mean()))\n",
    "    corrDataframe = corrDataframe.T\n",
    "    corrDataframe['average corr'] = corrDataframe.mean(numeric_only=True, axis=1)\n",
    "    print(corrDataframe.sort_values(by=['average corr'], ascending=False).to_string())\n",
    "\n",
    "    return bst\n",
    "\n",
    "def data_training_process(user_data_path, initial_features_path, model_save_path, rebalance_method = 'up', start_hour=6, model=xgb, param=None):\n",
    "    initial_features = load_pickle_file(initial_features_path)\n",
    "    users = load_pickle_file(user_data_path)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "\n",
    "    df, X, y = prepare_training_data(initial_features)\n",
    "\n",
    "    feature_columns = X.columns\n",
    "    print('There are {} Features'.format(len(feature_columns)))\n",
    "    #xgboost\n",
    "    if param == None:\n",
    "        param = {\n",
    "            'max_depth':3,\n",
    "            # Step size shrinkage used in update to prevents overfitting. \n",
    "            # After each boosting step, we can directly get the weights of new features, \n",
    "            # and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "            'eta': 0.1,\n",
    "            # Minimum loss reduction required to make a further partition on a leaf node of the tree. \n",
    "            # The larger gamma is, the more conservative the algorithm will be.\n",
    "            'gamma':10,\n",
    "            # Minimum sum of instance weight (hessian) needed in a child. \n",
    "            # If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, \n",
    "            # then the building process will give up further partitioning.\n",
    "            # The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "            'min_child_weight':10,\n",
    "            'silent': 1, # 0 means printing running messages, 1 means silent mode\n",
    "            'objective': 'binary:logistic',\n",
    "            'subsample': 0.9\n",
    "        }\n",
    "    param['nthread'] = cpu_count\n",
    "    param['eval_metric'] = ['auc']\n",
    "    num_boost_round = 1000\n",
    "    rebalance_method = 'up'\n",
    "\n",
    "#     xgb_model = train(df, X, y, model, param, 2, num_boost_round, rebalance_method)\n",
    "    \n",
    "  \n",
    "   \n",
    "    columns = list(df.columns)\n",
    "    columns.remove('label')\n",
    "    if rebalance_method == 'up':\n",
    "        df_rebalance = upsample(df)\n",
    "    if rebalance_method == 'down':\n",
    "        df_rebalance = downsample(df)\n",
    "    X = df_rebalance[columns]\n",
    "    y = df_rebalance[['label']]\n",
    "\n",
    "    # load JS visualization code to notebook\n",
    "    #shap.initjs()\n",
    "\n",
    "    # train XGBoost model\n",
    "\n",
    "    model = xgb.train(param, xgb.DMatrix(X, label=y), num_boost_round)\n",
    "    \n",
    "    with open(model_save_path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "\n",
    "    # explain the model's predictions using SHAP values\n",
    "    # (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "    #explainer = shap.TreeExplainer(model)\n",
    "    #shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # visualize the first prediction's explanation\n",
    "    #shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(source_id,target_id,features,network_simulation,current_time):\n",
    "    \n",
    "    if isinstance(network_simulation.loc[target_id,'source_candidates'],list) and len(network_simulation.loc[target_id,'source_candidates']) > 0 :\n",
    "        \n",
    "        source_candidates = sorted(network_simulation.loc[target_id,'source_candidates'])\n",
    "        nf = network_simulation.loc[source_candidates]\n",
    "        \n",
    "        sources = nf[nf['time_lapsed'] <= current_time].index.tolist()\n",
    "        \n",
    "        if len(sources) > 0:\n",
    "        \n",
    "            first_source_index = source_candidates[0]\n",
    "            first_source_row = network_simulation.loc[first_source_index]\n",
    "            first_source_seed_row = network_simulation.loc[first_source_row['seed_index']]\n",
    "\n",
    "            sources_dataframe = network_simulation.loc[sources]\n",
    "            degreeList = list(degree[i] for i in sources)\n",
    "            inDegreeList = list(in_degree[i] for i in sources)\n",
    "            outDegreeList = list(out_degree[i] for i in sources)\n",
    "            degreeList = list(network_simulation.loc[i, 'followers_count'] + network_simulation.loc[i, 'friends_count']  for i in sources)\n",
    "            timeList = [current_time - network_simulation.loc[x,'time_lapsed'] for x in sources]\n",
    "\n",
    "\n",
    "            last_source_index = sources[-1]    \n",
    "            try:\n",
    "                last_source_row = network_simulation.loc[last_source_index]\n",
    "                last_source_seed_row = network_simulation.loc[last_source_row['seed_index']]\n",
    "            except:\n",
    "                print(f\"target_index:{target_id}, last_source_row['seed_index'] : {last_source_row['seed_index']}\")\n",
    "                print(f\"last_source_index:{last_source_index}\")\n",
    "\n",
    "\n",
    "            #Extraction\n",
    "            #Columns which are added for simulation, but they are not used as features for model prediction\n",
    "\n",
    "            user_row = network_simulation.loc[target_id]\n",
    "\n",
    "\n",
    "            # UsM: User metadata                    \n",
    "\n",
    "            features.loc[target_id,'UsM_deltaDays0'] = first_source_row.user_created_days\n",
    "            features.loc[target_id,'UsM_statusesCount0'] = first_source_row.statuses_count\n",
    "            features.loc[target_id,'UsM_followersCount0'] = first_source_row.followers_count\n",
    "            features.loc[target_id,'UsM_favouritesCount0'] = first_source_row.favourites_count\n",
    "            features.loc[target_id,'UsM_friendsCount0'] = first_source_row.friends_count\n",
    "            features.loc[target_id,'UsM_listedCount0'] = first_source_row.listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserStatusesCount0'] = first_source_row.normalized_statuses_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFollowersCount0'] = first_source_row.normalized_followers_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFavouritesCount0'] = first_source_row.normalized_favourites_count\n",
    "            features.loc[target_id,'UsM_normalizedUserListedCount0'] = first_source_row.normalized_listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFriendsCount0'] = first_source_row.normalized_friends_count\n",
    "            features.loc[target_id,'UsM_deltaDays-1'] = last_source_row.user_created_days\n",
    "            features.loc[target_id,'UsM_statusesCount-1'] = last_source_row.statuses_count\n",
    "            features.loc[target_id,'UsM_followersCount-1'] = last_source_row.followers_count\n",
    "            features.loc[target_id,'UsM_favouritesCount-1'] = last_source_row.favourites_count\n",
    "            features.loc[target_id,'UsM_friendsCount-1'] = last_source_row.friends_count\n",
    "            features.loc[target_id,'UsM_listedCount-1'] = last_source_row.listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserStatusesCount-1'] = last_source_row.normalized_statuses_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFollowersCount-1'] = last_source_row.normalized_followers_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFavouritesCount-1'] = last_source_row.normalized_favourites_count\n",
    "            features.loc[target_id,'UsM_normalizedUserListedCount-1'] = last_source_row.normalized_listed_count\n",
    "            features.loc[target_id,'UsM_normalizedUserFriendsCount-1'] = last_source_row.normalized_friends_count\n",
    "            # TwM: Tweet metadata\n",
    "            features.loc[target_id,'TwM_t0'] = round(timeList[0], 1)\n",
    "            features.loc[target_id,'TwM_tSeed0'] = round(current_time - first_source_seed_row['time_lapsed'], 1)\n",
    "            features.loc[target_id,'TwM_t-1'] = round(timeList[-1], 1)\n",
    "            features.loc[target_id,'TwM_tSeed-1'] = round(current_time - last_source_seed_row['time_lapsed'], 1)\n",
    "            features.loc[target_id,'TwM_tCurrent'] = current_time\n",
    "            # Nw: Network\n",
    "            features.loc[target_id,'Nw_degree'] = degree[target_id]\n",
    "            features.loc[target_id,'Nw_inDegree'] = in_degree[target_id]\n",
    "            features.loc[target_id,'Nw_outDegree'] = out_degree[target_id]\n",
    "            features.loc[target_id,'Nw_degree0'] = degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_inDegree0'] = in_degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_outDegree0'] = out_degree[first_source_index]\n",
    "            features.loc[target_id,'Nw_degree-1'] = degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_inDegree-1'] = in_degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_outDegree-1'] = out_degree[last_source_index]\n",
    "            features.loc[target_id,'Nw_degreeSeed0'] = degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_inDegreeSeed0'] = in_degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_outDegreeSeed0'] = out_degree[int(first_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_degreeSeed-1'] = degree[int(last_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_inDegreeSeed-1'] = in_degree[int(last_source_row['seed_index'])]\n",
    "            features.loc[target_id,'Nw_outDegreeSeed-1'] = out_degree[int(last_source_row['seed_index'])]\n",
    "            # SNw: Spreading Network\n",
    "            features.loc[target_id,'SNw_nFriendsInfected'] = len(sources)\n",
    "            features.loc[target_id,'SNw_friendsInfectedRatio'] = safe_division(len(sources), user_row['friends_count'])\n",
    "            features.loc[target_id,'SNw_generation0'] = first_source_row['generation']\n",
    "            features.loc[target_id,'SNw_generation-1'] = last_source_row['generation']\n",
    "            features.loc[target_id,'SNw_timeSinceSeed0'] = first_source_row['time_since_seed']\n",
    "            features.loc[target_id,'SNw_timeSinceSeed-1'] = last_source_row['time_since_seed']\n",
    "\n",
    "            infected_dataframe = network_simulation[network_simulation.time_lapsed <= current_time]\n",
    "            total_nodes_infected = infected_dataframe.shape[0]\n",
    "            total_in_degree = sum(infected_dataframe.friends_count)\n",
    "            total_out_degree = sum(infected_dataframe.followers_count)\n",
    "\n",
    "            features.loc[target_id,'SNw_totalNodesInfected'] = total_nodes_infected\n",
    "            features.loc[target_id,'SNw_nodeInfectedCentrality'] = len(sources)/total_nodes_infected\n",
    "            features.loc[target_id,'SNw_totalInDegree'] = total_in_degree\n",
    "            features.loc[target_id,'SNw_totalOutDegree'] = total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality'] = in_degree[target_id]/total_in_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality0'] = in_degree[first_source_index]/total_in_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentrality-1'] = in_degree[last_source_index]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality'] = out_degree[target_id]/total_out_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality0'] = out_degree[first_source_index]/total_out_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentrality-1'] = out_degree[last_source_index]/total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentralitySeed0'] = in_degree[int(first_source_row['seed_index'])]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentralitySeed0'] = out_degree[int(first_source_row['seed_index'])]/total_out_degree\n",
    "            features.loc[target_id,'SNw_inDegreeCentralitySeed-1'] = in_degree[int(last_source_row['seed_index'])]/total_in_degree\n",
    "            features.loc[target_id,'SNw_outDegreeCentralitySeed-1'] = out_degree[int(last_source_row['seed_index'])]/total_out_degree\n",
    "            # Stat: Statistical\n",
    "            features.loc[target_id,'Stat_average_kOut'] = round(mean(degreeList), 1)\n",
    "            features.loc[target_id,'Stat_average_t'] = round(mean(timeList), 1)\n",
    "            features.loc[target_id,'Stat_average_deltaDays'] = sources_dataframe.user_created_days.mean()\n",
    "            features.loc[target_id,'Stat_average_statusesCount'] = sources_dataframe.statuses_count.mean()\n",
    "            features.loc[target_id,'Stat_average_followersCount'] = sources_dataframe.followers_count.mean()\n",
    "            features.loc[target_id,'Stat_average_favouritesCount'] = sources_dataframe.favourites_count.mean()\n",
    "            features.loc[target_id,'Stat_average_friendsCount'] = sources_dataframe.friends_count.mean()\n",
    "            features.loc[target_id,'Stat_average_listedCount'] = sources_dataframe.listed_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserStatusesCount'] = sources_dataframe.normalized_statuses_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFollowersCount'] = sources_dataframe.normalized_followers_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFavouritesCount'] = sources_dataframe.normalized_favourites_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserListedCount'] = sources_dataframe.normalized_listed_count.mean()\n",
    "            features.loc[target_id,'Stat_average_normalizedUserFriendsCount'] = sources_dataframe.normalized_friends_count.mean()\n",
    "            features.loc[target_id,'Stat_max_kOut'] = max(degreeList)\n",
    "            features.loc[target_id,'Stat_min_kOut'] = min(degreeList)\n",
    "    #processed_dataframe = pd.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "\n",
    "def simulation(features,dataset,network_simulation,current_time,model):\n",
    "    infected_users_indices = network_simulation[network_simulation['time_lapsed'].isnull() == False].index.values\n",
    "    print(\"there are \"+ str(len(infected_users_indices)) +\" infected users.\")\n",
    "    for i in infected_users_indices:\n",
    "        print(\"=====\"+str(i)+\"th/\"+str(len(infected_users_indices))+\" user=====\")\n",
    "        if isinstance(network_simulation.loc[i,'followers_list'],list):\n",
    "   \n",
    "            followers_indices = list(set(initial_dataset['id']).intersection(set(initial_dataset.loc[i].followers_list)))\n",
    "   \n",
    "            followers_indices = [list(initial_dataset['id']).index(x) for x in followers_indices]      \n",
    "#             followers_indices = [network_simulation[network_simulation['id'] == x].index.values.item() for x in network_simulation.loc[i].followers_list]\n",
    "#             print(followers_indices)\n",
    "            uninfected_followers_indices = [y for y in followers_indices if np.isnan(network_simulation.loc[y, 'time_lapsed']) == True]\n",
    "\n",
    "            if len(uninfected_followers_indices) > 0:\n",
    "                #print(f\"uninfected followers are there\")\n",
    "                for j in uninfected_followers_indices:\n",
    "                    #print(f\"j:{j}\")\n",
    "                    source_index = i\n",
    "                    target_index = j\n",
    "                    processed_dataframe = process_data(source_index,target_index,features,network_simulation,current_time)\n",
    "                    #print(f\"len processed_dataframe: {processed_dataframe[0,:]}\")\n",
    "                    try:\n",
    "                        valid_row = processed_dataframe.loc[[target_index]]\n",
    "                    except:\n",
    "                        print(f\"source_index:{source_index}\")\n",
    "                        print(f\"target_index:{target_index}\")\n",
    "                        processed_dataframe.to_csv(path+'processed_dataframe.csv')\n",
    "                        print(processed_dataframe.loc[[target_index]])\n",
    "                    #print(f\"(valid_row.columns):{list(valid_row.columns.values)}\")\n",
    "                    \n",
    "                    #for i in range(valid_row.columns):\n",
    "                        #print(f\"valid_row:{i}\")\n",
    "                    #valid = valid_row.drop(['user_id','infected_status','infection_time','followers_list','Nw_inDegree','Nw_outDegree'],axis=1)\n",
    "                    valid = valid_row.drop(['user_id','infected_status','infection_time','followers_list'],axis=1)\n",
    "                    #print(f\"list(valid.columns):{list(valid.columns)}\")\n",
    "                    valid = valid.astype('float64')\n",
    "\n",
    "                    columns = list(valid.columns)\n",
    "                    X = valid[columns]\n",
    "                    \n",
    "                    pre_data = xgb.DMatrix(X)\n",
    "                    #print(f\"pre_data.columns:{pre_data}\")\n",
    "                    infec = model.predict(pre_data)\n",
    "                    if infec > 0.5:\n",
    "                        print(\"user \"+str(j)+\" Infected\")\n",
    "                        network_simulation.loc[target_index,'time_lapsed'] = current_time\n",
    "                        network_simulation.loc[target_index,'source_index'] = source_index  #why source_index not first_source_index\n",
    "                        network_simulation.loc[target_index,'seed_index'] = network_simulation.loc[source_index,'seed_index']\n",
    "                        network_simulation.loc[target_index,'generation'] = network_simulation.loc[source_index,'generation'] + 1\n",
    "                        seed_index = network_simulation.loc[target_index,'seed_index']\n",
    "                        network_simulation.loc[target_index,'time_since_seed'] = current_time - network_simulation.loc[seed_index,'time_lapsed']\n",
    "                        followers_of_node = network_simulation.loc[target_index,'followers_list'] \n",
    "                        if isinstance(followers_of_node,list):\n",
    "                            for f in followers_of_node:\n",
    "                                if np.isnan(network_simulation[network_simulation['id'] == f]['time_lapsed'].values):\n",
    "                                    follower_index = network_simulation[network_simulation['id'] == f].index.values\n",
    "                                    try:\n",
    "                                        list(network_simulation.loc[follower_index,'source_candidates'].values).append(f)\n",
    "                                    except:\n",
    "                                        print(f\"source_candidates:{network_simulation.loc[follower_index,'source_candidates'].values}\")\n",
    "                                        print(f\"f:{f}\")\n",
    "\n",
    "    return network_simulation\n",
    "\n",
    "\n",
    "def simulation_process(current_time, total_time_duration, interval, features_path, network_path, user_path, model_path, simulation_result_path):\n",
    "    initial_features = load_pickle_file(features_path)\n",
    "    initial_dataset = load_pickle_file(network_path)\n",
    "    users = load_pickle_file(user_path)\n",
    "    users.reset_index(drop =True , inplace =True)\n",
    "\n",
    "\n",
    "    model = load_pickle_file(model_path)\n",
    "#     total_time_duration = 540\n",
    "#     interval = 30\n",
    "    #current_time = 360\n",
    "#     current_time = 360\n",
    "    #current_time = 480\n",
    "    #current_time = 540\n",
    "    #current_time = 720\n",
    "\n",
    "\n",
    "    features = initial_features\n",
    "    network_simulation = initial_dataset\n",
    "    id_set = set(network_simulation['id'].tolist())\n",
    "\n",
    "    print(\"Simulation started\")\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    while current_time < total_time_duration:\n",
    "        print(f\"current_time:{current_time}\")\n",
    "        print(\"=================================================\")\n",
    "        #network_simulation = simulation(features,initial_dataset,network_simulation,current_time)\n",
    "        network_simulation = simulation(features,initial_dataset,network_simulation,current_time, model)\n",
    "        #print(f\"current_time:{current_time}\")\n",
    "        current_time += interval\n",
    "\n",
    "    print(f\"Simulation finished after {round((time.time() - start_time)/60,2)} minutes\")\n",
    "#     network_simulation.to_csv(simulation_result_path,index=False)\n",
    "\n",
    "    simulation_comparison = pd.concat([network_simulation['time_lapsed'],users['time_lapsed']],axis = 1)\n",
    "    column_names = ['nw_time_lapsed','usr_time_lapsed']\n",
    "    simulation_comparison.columns = column_names\n",
    "\n",
    "    simulation_comparison['both_infected'] = simulation_comparison.apply(lambda x: 1 if (np.isnan(x['nw_time_lapsed']) == False)\n",
    "                                                                              & (np.isnan(x['usr_time_lapsed']) == False)\n",
    "                                                                         & (x['usr_time_lapsed'] > current_time)\n",
    "                                                                         else 0,axis = 1)\n",
    "\n",
    "    true_positive = simulation_comparison['both_infected'].sum()\n",
    "    total_predicted_positive = len(simulation_comparison[np.isnan(simulation_comparison['nw_time_lapsed']) == False])\n",
    "    precision = true_positive/total_predicted_positive\n",
    "    total_actual_positive = len(simulation_comparison[np.isnan(simulation_comparison['usr_time_lapsed']) == False])\n",
    "    recall = true_positive/total_actual_positive\n",
    "    f1_score = (2*(precision * recall))/(precision + recall)\n",
    "    print('true_positive: '+str(true_positive))\n",
    "    print('total_predicted_positive: '+str(total_predicted_positive))\n",
    "    print('precision: '+str(precision))\n",
    "    print('recall: '+str(recall))\n",
    "    print('f1_score: '+str(f1_score))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\users.dat\n",
      "Loaded 5973 entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                               | 57/5973 [00:00<01:07, 87.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ae80f7e5ed51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mstart_hour\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdata_preparation_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollower_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfriend_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_hour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0minitial_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e3e6f4a28908>\u001b[0m in \u001b[0;36mdata_preparation_process\u001b[1;34m(path_of_user_data, path_of_follower_list, path_of_friend_list, feature_file_path, network_file_path, start_hour)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[0mend_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5973\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_hour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m     \u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork_simulation_merge_friends_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_of_friend_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnetwork_simulation_merge_followers_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_of_follower_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e3e6f4a28908>\u001b[0m in \u001b[0;36mprocess_data\u001b[1;34m(users, network_simulation, start_index, end_index, start_hour)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SNw_timeSinceSeed-1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_source_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time_since_seed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[0minfected_dataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnetwork_simulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_lapsed\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                 \u001b[0mtotal_nodes_infected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfected_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[0mtotal_in_degree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfected_dataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfriends_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2789\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2790\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2791\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2793\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2842\u001b[0m         \u001b[1;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2843\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2844\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2845\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# data_1 = [[10, 20, 30, 40, 50], [11, 12, 13, 14, 15], [21, 22, 23, 24, 25], [31, 32, 33, 34, 35], [41, 42, 43, 44, 45]]\n",
    "# # date_range1 = pd.date_range(start=\"20190301\", periods=5)\n",
    "# df1 = pd.DataFrame(data=data_1, \n",
    "#                   columns=['A', 'B', 'C', 'D', 'E'])\n",
    "# print(df1)\n",
    "# def tet(df):\n",
    "#     df['newco']=''\n",
    "# tet(df1)\n",
    "# print(df1)\n",
    "user_data = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "follower_list='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\friends.dat'\n",
    "friend_list='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\followers.dat'\n",
    "feature_file_path='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "network_file_path='F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl'\n",
    "start_hour= 6\n",
    "    \n",
    "data_preparation_process(user_data, follower_list, friend_list, feature_file_path, network_file_path, start_hour)\n",
    "\n",
    "initial_dataset = load_pickle_file('F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl')\n",
    "users = load_pickle_file(user_data)  \n",
    "users.reset_index(drop =True , inplace =True)\n",
    "# for i in range(0,5973):\n",
    "#     print(i,users.at[i, 'time_lapsed'])\n",
    "# for i in range(0,5973):\n",
    "#     print(i,initial_dataset.at[i, 'source_candidates'])\n",
    "# print(initial_dataset)\n",
    "print(initial_dataset.at[2811, 'followers_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data training Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\feature_6_hour.pkl\n",
      "Loaded 5973 entries\n",
      "Loading data file from F:\\twitter_data\\givenchy\\pickle\\users.dat\n",
      "Loaded 5973 entries\n",
      "[Original] data counts, with uninfected (0): 2569, infected (1): 3404\n",
      "There are 88 Features\n",
      "1.0    2569\n",
      "0.0    2569\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEqCAYAAAD58oAeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAezElEQVR4nO3df7xVdZ3v8dc7RMAUfwB6kYOCgQqYkhwZblbaJYNMxXqY4VXBsaJIy2nsjj9qCmcuM947/bgPr6FDk4HFlcgyzAELvTrkhNCBa/LDGChRDhAeMRTyF+Dn/rG+B7fHfc5Z+5zD3sB6Px+P/Thrf9b3u9Z3cx68z9rftfbaigjMzKwY3lHrAZiZWfU49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrEAc+tZpkm6W9C9d3TbHtkLSkK7YVq1IulPS33bRtk6QtFNSt/T8UUmf7optp+0tlDS5q7ZntSFfp2+lJF0FXA+8C3gJuA+4KSK213Jc5UgKYGhErC+z7lFgDLALCGAd8GPg2xHxWme3n7P/BuA4YDewB1gD3A3MjIg3OrCtT0fEQxX0eRT4YURU/EdW0jRgSERcUWlf27/5SN/2knQ98D+A/wYcSRaaJwKLJB3aSp9DqjfCil0bEUcA/cn+kE0EFkhSFcdwYRrDicCtwA3A97p6J/v578H2JxHhhx8AvYGdwKUt6ocDzwFXp+fTgHuBH5K9E/h0qv2wpM8k4BlgG/C3wAbgQyX9f5iWB5EdhU8GngWeB75Ssp3RwBJgO7AFuB04tGR9kB2Nlns9j5IdGZfWTgBeBi5ob/vA4rT9P6d/l08CRwMPAE3An9JyXRv/pntfd4vX9AZwWno+C/jvablv2uZ24AXgV2QHZj9IfV5JY/mbkn+7T6V/u8UltUNK/g3+EVgGvAjMB45J684FGsuNFxgPvE72Lmkn8NuW/6ZpXF9Nv+fnyN7BHJnn9+pHbR8+0rdm7wV6Aj8tLUbETmAhcF5JeQJZ8B8FzCltL2k4MAO4nOwI+0hgQDv7fh9wCjAW+JqkYam+B/gSWRj+57T+8xW+rtLX8izQALy/ve1HxAdSmzMi4vCI+BFZ0H2f7Kj9BLIQvr3CMSwDGkvGUOr6tK4f2bTQzVmXuJIsPC9MY/mfJX3OAYYB41rZ5STgauB4smmm23KM8UHgH4Afpf2dUabZVenxQeAksoODlv8Wrf1erYYc+tasL/B8ROwus25LWt9sSUT8LCLeiIhXWrS9BPh5RDwWEa8DXyM76mvLLRHxSkT8FvgtcAZARCyPiMcjYndEbAD+mSzkOmMzcExHth8R2yLiJxHxckTsAKZ3cDx7x9DCLrI/lCdGxK6I+FVEtPdvNy0i/lzm99DsBxGxKiL+TPau69LmE72ddDnwrYj4QzowuAmY2GKaqezv1WrLoW/Nngf6tjI33D+tb7axje0cX7o+Il4mm+Zpyx9Lll8mO2pE0smSHpD0R0kvkR199i23gQoMIJs6qXj7kg6T9M+SnkntFwNHdSBE946hhX8C1gO/lPQHSTfm2FZbv4uW658ButP5f0PIfs/PtNj2IWTvUJqV/b1abTn0rdkS4DXg46VFSe8EPgI8XFJu6+hzC1BX0r8X0KeDY7oD+B3ZFTS9yaY7OnwSVtJAYBTZXHlHtn892XTFX6T2zVNAucck6Syy0H+s5bqI2BER10fEScCFwF9LGtu8upVNtvdOYGDJ8glk7yaeJztXcVjJuLqRTSvl3e5msmmu0m3vBra2089qzKFvAETEi8AtwP+WNF5Sd0mDyC5zbCQ7mZjHvcCFkt6brvi5hY4H9RFkJ4t3SjoVmNqRjaQj9HPITmQuAxbk3P5Wsvnq0vG8AmyXdAzw9QrG0FvSBcBcshPZK8u0uUDSkHR10Utk5xz2tDKWvK6QNFzSYcDfAfdGxB7gP4Cekj4qqTvZSdkeJf22AoMktZYR9wBfkjRY0uG8eQ6g3PSg7Ucc+rZXOkF4M/ANstBZSjY9MDZyXtseEauBL5CF2xZgB9nVHbn6t/Bl4L+mbXwX+FGF/W+XtIMswP4X8BNgfLx5jXx7258GzJa0XdKlaRu9yI6UHwcezDGGn6cxbAS+AnwL+MtW2g4FHiK7YmYJMCMiHk3r/hH4ahrLl3Pst9kPyK4Q+iPZifovwt4/8p8H/gXYRHbk31jS78fp5zZJK8ps96607cXA08CrZL9328/5w1m2T6WjwO1kUyhP13o8ZkXnI33rcpIuTFMq7yR717CS7BpwM6sxh77tCxPITvRtJpuymJjj0kMzqwJP75iZFYiP9M3MCsShb2ZWIPv9nfn69u0bgwYNqvUwzMwOKMuXL38+Ivq1rLcb+pJ6kl2L2yO1vzcivp7ut/0ZsjsOAtwcEQtSn5vI7v63B/hiRPwi1UeRXTPci+wDMte1d4Jv0KBBNDQ05HmNZmaWSHqmXD3Pkf5rwH+JiJ3pk3uPSVqY1n07Ir7RYkfDye5bPoLs/hwPSTo5fQrwDmAK2QdbFpDdwnUhZmZWFe3O6UdmZ3raPT3aOjqfAMyNiNfSh3HWA6Ml9Qd6R8SSdHR/N3Bx54ZvZmaVyHUiV1I3SU+QfZx+UUQsTauulfSkpLskHZ1qA3jrnf0aU20Ab/2Yd3O93P6mSGqQ1NDU1FSuiZmZdUCuE7lpamakpKOA+ySdRjZV8/dkR/1/D3yT7Msayt1cK9qol9vfTGAmQH19vT9IYGZdateuXTQ2NvLqq6/Weiid1rNnT+rq6ujevXuu9hVdvRMR29OXLY8vncuX9F2yr3mD7Ai+9HaudWSfzGyk5Ja7JXUzs6pqbGzkiCOOYNCgQVT3K5O7VkSwbds2GhsbGTx4cK4+7U7vSOqXjvCb743+IeB3aY6+2ceAVWn5frJv0OkhaTDZx/CXRcQWYIekMenWsZPIbnVrZlZVr776Kn369DmgAx9AEn369KnoHUueI/3+ZLeX7Ub2R2JeRDwg6QeSRpJN0WwAPgvZrXUlzQPWkH2pwjVpegiy+5XPIrtkcyG+csfMauRAD/xmlb6OdkM/Ip4E3lOmfmUbfaaTfX9oy3oDcFpFIzQz208cfvjh7Ny5s9X1GzZs4IILLmDVqlWttmnpqquu4oILLuCSSy7piiG2a7//RK7ZfmvakbUewcFl2ou1HkEh+N47ZmYV2rlzJ2PHjuXMM8/k3e9+N/Pnv3l6cvfu3UyePJnTTz+dSy65hJdffhmA5cuXc8455zBq1CjGjRvHli1bajJ2h76ZWYV69uzJfffdx4oVK3jkkUe4/vrrab6jzNq1a5kyZQpPPvkkvXv3ZsaMGezatYsvfOEL3HvvvSxfvpyrr76ar3zlKzUZu6d3zMwqFBHcfPPNLF68mHe84x1s2rSJrVu3AjBw4EDOPvtsAK644gpuu+02xo8fz6pVqzjvvPMA2LNnD/379291+/uSQ9/MrEJz5syhqamJ5cuX0717dwYNGrT3ssmWV9NIIiIYMWIES5YsqcVw38LTO2ZmFXrxxRc59thj6d69O4888gjPPPPmDS2fffbZveF+zz338L73vY9TTjmFpqamvfVdu3axevXqmozdoW9mVqHLL7+choYG6uvrmTNnDqeeeuredcOGDWP27NmcfvrpvPDCC0ydOpVDDz2Ue++9lxtuuIEzzjiDkSNH8utf/7omY/f0jplZTs3X6Pft27fVqZo1a9aUrY8cOZLFixe/rT5r1qwuG18ePtI3MysQh76ZWYE49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrAYefPBBTjnlFIYMGcKtt976tvURwRe/+EWGDBnC6aefzooVK7pkv75O38wKb9CN/9ql29tw60fbXL9nzx6uueYaFi1aRF1dHWeddRYXXXQRw4cP39tm4cKFrFu3jnXr1rF06VKmTp3K0qVLOz02H+mbmVXZsmXLGDJkCCeddBKHHnooEydOfMvtmQHmz5/PpEmTkMSYMWPYvn17l9yO2aFvZlZlmzZtYuDAgXuf19XVsWnTporbdIRD38ysyprvvV+q5d0587TpCIe+mVmV1dXVsXHjxr3PGxsbOf744ytu0xEOfTOzKjvrrLNYt24dTz/9NK+//jpz587loosuekubiy66iLvvvpuI4PHHH+fII4/ski9eaTf0JfWUtEzSbyWtlnRLqh8jaZGkdenn0SV9bpK0XtJaSeNK6qMkrUzrblNXvFcxMzvAHHLIIdx+++2MGzeOYcOGcemllzJixAjuvPNO7rzzTgDOP/98TjrpJIYMGcJnPvMZZsyY0SX7Vrl5o7c0yIL5nRGxU1J34DHgOuDjwAsRcaukG4GjI+IGScOBe4DRwPHAQ8DJEbFH0rLU93FgAXBbRCxsa//19fXR0NDQuVdpti9MO7LWIzi4THuxart66qmnGDZsWNX2t6+Vez2SlkdEfcu27R7pR2Zneto9PQKYAMxO9dnAxWl5AjA3Il6LiKeB9cBoSf2B3hGxJLK/NHeX9DEzsyrINacvqZukJ4DngEURsRQ4LiK2AKSfx6bmA4CNJd0bU21AWm5ZNzOzKskV+hGxJyJGAnVkR+2ntdG83Dx9tFF/+wakKZIaJDU0NTXlGaKZmeVQ0dU7EbEdeBQYD2xNUzakn8+lZo3AwJJudcDmVK8rUy+3n5kRUR8R9f369atkiGZm1oY8V+/0k3RUWu4FfAj4HXA/MDk1mww0f4b4fmCipB6SBgNDgWVpCmiHpDHp5PCkkj5mZlYFeW641h+YLakb2R+JeRHxgKQlwDxJnwKeBT4BEBGrJc0D1gC7gWsiYk/a1lRgFtALWJgeZmZWJe2GfkQ8CbynTH0bMLaVPtOB6WXqDUBb5wPMzArh6quv5oEHHuDYY49l1apVb1sfEVx33XUsWLCAww47jFmzZnHmmWd2er++tbKZWVd/5iLHZw6uuuoqrr32WiZNmlR2vW+tbGZ2EPnABz7AMccc0+p631rZzKxAfGtlM7MC8a2VzcwKxLdWNjMrkH11a2VfvWNmVgOXXXYZjz76KM8//zx1dXXccsst7Nq1C4DPfe5znH/++SxYsIAhQ4Zw2GGH8f3vf79L9uvQNzOr4m2dm91zzz1trpfEd77znS7fr6d3zMwKxKFvZlYgDn0zswJx6JtZIbX3VbEHikpfh0PfzAqnZ8+ebNu27YAP/ohg27Zt9OzZM3cfX71jZoVTV1dHY2MjB8M38/Xs2ZO6urr2GyYOfTMrnO7duzN48OBaD6MmPL1jZlYgDn0zswJx6JuZFYhD38ysQBz6ZmYF4tA3MysQh76ZWYG0G/qSBkp6RNJTklZLui7Vp0naJOmJ9Di/pM9NktZLWitpXEl9lKSVad1t6orv/jIzs9zyfDhrN3B9RKyQdASwXNKitO7bEfGN0saShgMTgRHA8cBDkk6OiD3AHcAU4HFgATAeWNg1L8XMzNrT7pF+RGyJiBVpeQfwFDCgjS4TgLkR8VpEPA2sB0ZL6g/0joglkd3w4m7g4k6/AjMzy62iOX1Jg4D3AEtT6VpJT0q6S9LRqTYA2FjSrTHVBqTllvVy+5kiqUFSw8Fwbwwzs/1F7tCXdDjwE+CvIuIlsqmadwEjgS3AN5ublukebdTfXoyYGRH1EVHfr1+/vEM0M7N25Ap9Sd3JAn9ORPwUICK2RsSeiHgD+C4wOjVvBAaWdK8DNqd6XZm6mZlVSZ6rdwR8D3gqIr5VUu9f0uxjwKq0fD8wUVIPSYOBocCyiNgC7JA0Jm1zEjC/i16HmZnlkOfqnbOBK4GVkp5ItZuByySNJJui2QB8FiAiVkuaB6whu/LnmnTlDsBUYBbQi+yqHV+5Y2ZWRe2GfkQ8Rvn5+AVt9JkOTC9TbwBOq2SAZmbWdfyJXDOzAnHom5kViEPfzKxAHPpmZgXi0DczKxCHvplZgTj0zcwKxKFvZlYgDn0zswJx6JuZFYhD38ysQBz6ZmYF4tA3MysQh76ZWYE49M3MCiTPl6jYQWLQjf9a6yEcVDb0rPUIzCrnI30zswJx6JuZFYhD38ysQBz6ZmYF4tA3MyuQdkNf0kBJj0h6StJqSdel+jGSFklal34eXdLnJknrJa2VNK6kPkrSyrTuNknaNy/LzMzKyXOkvxu4PiKGAWOAayQNB24EHo6IocDD6Tlp3URgBDAemCGpW9rWHcAUYGh6jO/C12JmZu1oN/QjYktErEjLO4CngAHABGB2ajYbuDgtTwDmRsRrEfE0sB4YLak/0DsilkREAHeX9DEzsyqoaE5f0iDgPcBS4LiI2ALZHwbg2NRsALCxpFtjqg1Iyy3r5fYzRVKDpIampqZKhmhmZm3IHfqSDgd+AvxVRLzUVtMytWij/vZixMyIqI+I+n79+uUdopmZtSNX6EvqThb4cyLip6m8NU3ZkH4+l+qNwMCS7nXA5lSvK1M3M7MqyXP1joDvAU9FxLdKVt0PTE7Lk4H5JfWJknpIGkx2wnZZmgLaIWlM2uakkj5mZlYFeW64djZwJbBS0hOpdjNwKzBP0qeAZ4FPAETEaknzgDVkV/5cExF7Ur+pwCygF7AwPczMrEraDf2IeIzy8/EAY1vpMx2YXqbeAJxWyQDNzKzr+BO5ZmYF4tA3MysQh76ZWYE49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrEAc+mZmBeLQNzMrEIe+mVmBOPTNzArEoW9mViAOfTOzAnHom5kViEPfzKxAHPpmZgXi0DczKxCHvplZgTj0zcwKxKFvZlYg7Ya+pLskPSdpVUltmqRNkp5Ij/NL1t0kab2ktZLGldRHSVqZ1t0mSV3/cszMrC15jvRnAePL1L8dESPTYwGApOHARGBE6jNDUrfU/g5gCjA0Pcpt08zM9qF2Qz8iFgMv5NzeBGBuRLwWEU8D64HRkvoDvSNiSUQEcDdwcUcHbWZmHdOZOf1rJT2Zpn+OTrUBwMaSNo2pNiAtt6ybmVkVdTT07wDeBYwEtgDfTPVy8/TRRr0sSVMkNUhqaGpq6uAQzcyspQ6FfkRsjYg9EfEG8F1gdFrVCAwsaVoHbE71ujL11rY/MyLqI6K+X79+HRmimZmV0aHQT3P0zT4GNF/Zcz8wUVIPSYPJTtgui4gtwA5JY9JVO5OA+Z0Yt5mZdcAh7TWQdA9wLtBXUiPwdeBcSSPJpmg2AJ8FiIjVkuYBa4DdwDURsSdtairZlUC9gIXpYWZmVdRu6EfEZWXK32uj/XRgepl6A3BaRaMzM7Mu5U/kmpkViEPfzKxAHPpmZgXi0DczKxCHvplZgTj0zcwKxKFvZlYgDn0zswJx6JuZFYhD38ysQBz6ZmYF4tA3MysQh76ZWYE49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrEAc+mZmBeLQNzMrEIe+mVmBOPTNzAqk3dCXdJek5yStKqkdI2mRpHXp59El626StF7SWknjSuqjJK1M626TpK5/OWZm1pY8R/qzgPEtajcCD0fEUODh9BxJw4GJwIjUZ4akbqnPHcAUYGh6tNymmZntY+2GfkQsBl5oUZ4AzE7Ls4GLS+pzI+K1iHgaWA+MltQf6B0RSyIigLtL+piZWZV0dE7/uIjYApB+HpvqA4CNJe0aU21AWm5ZL0vSFEkNkhqampo6OEQzM2upq0/klpunjzbqZUXEzIioj4j6fv36ddngzMyKrqOhvzVN2ZB+PpfqjcDAknZ1wOZUrytTNzOzKupo6N8PTE7Lk4H5JfWJknpIGkx2wnZZmgLaIWlMumpnUkkfMzOrkkPaayDpHuBcoK+kRuDrwK3APEmfAp4FPgEQEaslzQPWALuBayJiT9rUVLIrgXoBC9PDzMyqqN3Qj4jLWlk1tpX204HpZeoNwGkVjc7MzLqUP5FrZlYgDn0zswJx6JuZFYhD38ysQBz6ZmYF4tA3MysQh76ZWYE49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrEAc+mZmBeLQNzMrEIe+mVmBOPTNzArEoW9mViAOfTOzAnHom5kViEPfzKxAHPpmZgXSqdCXtEHSSklPSGpItWMkLZK0Lv08uqT9TZLWS1oraVxnB29mZpXpiiP9D0bEyIioT89vBB6OiKHAw+k5koYDE4ERwHhghqRuXbB/MzPLaV9M70wAZqfl2cDFJfW5EfFaRDwNrAdG74P9m5lZKzob+gH8UtJySVNS7biI2AKQfh6b6gOAjSV9G1PNzMyq5JBO9j87IjZLOhZYJOl3bbRVmVqUbZj9AZkCcMIJJ3RyiGZm1qxTR/oRsTn9fA64j2y6Zquk/gDp53OpeSMwsKR7HbC5le3OjIj6iKjv169fZ4ZoZmYlOhz6kt4p6YjmZeDDwCrgfmByajYZmJ+W7wcmSuohaTAwFFjW0f2bmVnlOjO9cxxwn6Tm7fyfiHhQ0m+AeZI+BTwLfAIgIlZLmgesAXYD10TEnk6N3szMKtLh0I+IPwBnlKlvA8a20mc6ML2j+zQzs87xJ3LNzArEoW9mViAOfTOzAnHom5kViEPfzKxAHPpmZgXi0DczKxCHvplZgTj0zcwKxKFvZlYgDn0zswJx6JuZFYhD38ysQBz6ZmYF4tA3MysQh76ZWYE49M3MCsShb2ZWIA59M7MCceibmRWIQ9/MrEAc+mZmBVL10Jc0XtJaSesl3Vjt/ZuZFVlVQ19SN+A7wEeA4cBlkoZXcwxmZkVW7SP90cD6iPhDRLwOzAUmVHkMZmaFdUiV9zcA2FjyvBH4i5aNJE0BpqSnOyWtrcLYzCoi6As8X+txHDRuUa1HcLA5sVyx2qFf7rcabytEzARm7vvhmHWcpIaIqK/1OMwqUe3pnUZgYMnzOmBzlcdgZlZY1Q793wBDJQ2WdCgwEbi/ymMwMyusqk7vRMRuSdcCvwC6AXdFxOpqjsGsC3kK0g44injblLqZmR2k/IlcM7MCceibmRWIQ9/MrEAc+mZmBeLQN8tB0pGSbpX0O0nb0uOpVDuq1uMzy8uhb5bPPOBPwLkR0Sci+gAfTLUf13RkZhXwJZtmOUhaGxGnVLrObH/jI32zfJ6R9DeSjmsuSDpO0g289SaCZvs1h75ZPp8E+gD/JukFSS8AjwLHAJfWcmBmlfD0jplZgfhI36yTJJ1Z6zGY5eXQN+u8qbUegFlent4xMyuQan9zltkBS5LIvud5ANk3vm0GloWPnOwA4iN9sxwkfRiYAawDNqVyHTAE+HxE/LJWYzOrhEPfLAdJTwEfiYgNLeqDgQURMawmAzOrkE/kmuVzCNl3PLe0Cehe5bGYdZjn9M3yuQv4jaS5vPkJ3BPIPrT1vZqNyqxCnt4xy0nSMGAC2YlckR353x8Ra2o6MLMKOPTNOkhSn4jYVutxmFXCc/pmOaT75vdNy6Mk/QF4XNIzks6p8fDMcnPom+Xz0Yh4Pi1/A/hkRAwFzgO+WbthmVXGoW+WT3dJzRc+9IqI3wBExH8APWo3LLPKeE7fLAdJXwAuBG4FPgAcBfwUGAucFBFX1nB4Zrk59M1yknQu2c3VTia73Hkj8DPg+xGxq4ZDM8vN1+mb5fcy8I2I+I2kEcB4oNGBbwcSH+mb5SDp68BHyA6UFpHdeO3fgA8Bv4iI6TUcnlluDn2zHCStBEaSnbT9I1AXES9J6gUsjYjTazpAs5x89Y5ZPrsjYk9EvAz8PiJeAoiIV4A3ajs0s/wc+mb5vC7psLQ8qrko6Ugc+nYA8fSOWQ6SekTEa2XqfYH+EbGyBsMyq5hD38ysQDy9Y2ZWIA59M7MCceibmRWIQ98OOJL6SHoiPf4oaVPJ80Nb6TNNUkgaUlL7UqrVt7Gvm1s8/0+S5kr6vaQ1khZIOrnrXl37JJ0r6b3V3KcdPBz6dsCJiG0RMTIiRgJ3At9ufh4Rr7fRdSUwseT5JUB733q1N/QlCbgPeDQi3hURw9P64/KOXVK3tp7ndC7g0LcOcejbweAdkpYDSDojHb2fkJ7/vuT6+p+Rfd0hkk4CXgSaWtuopFuBXukdxBzgg8CuiLizuU1EPBERv0pH3w+U9L1d0lVpeYOkr0l6DPhEmecflrRE0gpJP5Z0eEm/W1J9paRTJQ0CPgd8KY3r/V3xD2jF4dC3g8EbQE9JvYH3Aw3A+yWdCDyXPkUL8BKwUdJpwGXAj9raaETcCLyS3kFcDpwGLO/gGF+NiPdFxNzS58BDwFeBD0XEmWnsf13S7/lUvwP4ckRs4K3vbn7VwfFYQfkum3aw+DVwNtm97v+B7A6YAlqG4lyyKZ5xZPfC/8sqja/lH5jm52OA4cC/Z7NHHAosKWn30/RzOfDxfTlAKwaHvh0sfkV2lH8iMB+4AQjggRbtfg78E9CQbphWyT5Wk50HKGc3b33n3LPF+j+38lzAooi4rJXtNn8KeA/+/2pdwNM7drBYDFwBrIuIN4AXgPOBfy9tlG6QdgOQ91bIuyR1T8v/F+gh6TPNKyWdlb4Y/RlguKQe6X48Y3Nu/3Hg7OariiQdluNqoB3AETm3b/YWDn07KKS5bsjCH+AxYHtE/KlM27kRsSLnpmcCT0qaE9k9Sz4GnJdOEK8GpgGbI2IjMA94EpgD/L+c424CrgLukfQk2R+BU9vp9nPgYz6Rax3he++YmRWIj/TNzArEJ4bMAElLyb4Vq9SVvmWyHWw8vWNmViCe3jEzKxCHvplZgTj0zcwKxKFvZlYgDn0zswL5/+RG/IMyZeKZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_features_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "user_data_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "model_save_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\trained_model.dat'\n",
    "model = xgb\n",
    "data_training_process(user_data_path, initial_features_path, model_save_path, rebalance_method = 'up')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\network_simulation_6_hour.pkl'\n",
    "initial_features_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\feature_6_hour.pkl'\n",
    "user_data_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\users.dat'\n",
    "model_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\trained_model.dat'\n",
    "simulation_result_path = 'F:\\\\twitter_data\\\\givenchy\\\\pickle\\\\simulation_result_6_hour.pkl'\n",
    "total_time_duration = 540\n",
    "interval = 30\n",
    "current_time = 360\n",
    "\n",
    "in_degree = list(initial_dataset.friends_count)\n",
    "out_degree = list(initial_dataset.followers_count)\n",
    "degree = (users.friends_count+users.followers_count).tolist()\n",
    "\n",
    "\n",
    "simulation_process(current_time, total_time_duration, interval, initial_features_path, network_path, user_path, model_path, simulation_result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
